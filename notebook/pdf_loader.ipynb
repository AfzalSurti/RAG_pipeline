{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f596c650",
   "metadata": {},
   "source": [
    "### RAG Pipeline -  Data Ingestion To Vector DB pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0b34450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-text-splitters\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b76b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 PDF files in ../data\n",
      "\n",
      "processing:1_Final_DFA.pdf\n",
      "loaded 160 pages from the 1_Final_DFA.pdf\n",
      "\n",
      "processing:01_Introduction_to_Attention_Mechanisms.pdf\n",
      "loaded 3 pages from the 01_Introduction_to_Attention_Mechanisms.pdf\n",
      "\n",
      "processing:02_Transformer_and_Self_Attention.pdf\n",
      "loaded 2 pages from the 02_Transformer_and_Self_Attention.pdf\n",
      "\n",
      "processing:03_Advanced_Attention_Variants.pdf\n",
      "loaded 3 pages from the 03_Advanced_Attention_Variants.pdf\n",
      "\n",
      "total documents loaded: 168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_all_pdfs(pdf_directory):\n",
    "    all_documents=[]\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        print(f\"\\nprocessing:{pdf.name}\")\n",
    "        try:\n",
    "            loader=PyMuPDFLoader(str(pdf))\n",
    "            documents=loader.load()\n",
    "\n",
    "            for page_i,doc in enumerate(documents):\n",
    "                doc.metadata[\"source_file\"]=pdf.name\n",
    "                doc.metadata[\"source_path\"]=str(pdf)\n",
    "                doc.metadata[\"file_type\"]=\"pdf\"\n",
    "                doc.metadata[\"page\"]=doc.metadata.get(\"page\", page_i+1) # if the page number is not available, set it to the page index + 1\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"loaded {len(documents)} pages from the {pdf.name}\")\n",
    "            doc.metadata[\"doc_id\"]=pdf.stem # set the doc_id to the file name without extension\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error loading {pdf.name}: {e}\")\n",
    "\n",
    "    print(f\"\\ntotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents=load_all_pdfs(\"../data\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71be1119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\\nfor the Transformer architecture and the modern era of large language models.\\n2. The Core Intuition\\nThink of attention like a search engine. You have a query (what you're looking for), a set of keys (labels\\nor identifiers for stored information), and values (the actual information stored). The attention\\nmechanism computes a similarity score between the query and each key, converts these scores into\\nprobabilities using softmax, and returns a weighted sum of the values.\\nThe general attention formula is:\\nAttention(Q, K, V) = softmax(QKT / sqrt(d_k)) * V\\nWhere Q is the query matrix, K is the key matrix, V is the value matrix, and d_k is the dimension of the\\nkeys. The division by sqrt(d_k) prevents the dot products from growing too large in magnitude, which\\nwould push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\\nSoft attention computes a weighted average over all input positions — it is fully differentiable and can\\nbe trained end-to-end with backpropagation. Hard attention, on the other hand, selects a single input\\nposition at each step stochastically. While hard attention can be more efficient, it requires reinforcement\\nlearning techniques to train since it is not differentiable.\\n3.2 Self-Attention (Intra-Attention)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='Self-attention allows a sequence to attend to itself. Every token in the input sequence computes\\nattention scores with every other token, capturing relationships within the same sequence. This is\\nextremely powerful for tasks like understanding pronoun reference, long-range dependencies, and\\nsyntactic structure. Self-attention is the cornerstone of the Transformer architecture.\\n3.3 Cross-Attention\\nCross-attention is used when queries come from one sequence (e.g., the decoder) and keys/values\\ncome from another sequence (e.g., the encoder output). This is the classic encoder-decoder attention\\nused in machine translation and image captioning tasks.\\n3.4 Multi-Head Attention\\nInstead of performing a single attention function, multi-head attention runs h parallel attention\\noperations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation\\nsubspaces at different positions.\\nMultiHead(Q,K,V) = Concat(head_1, ..., head_h) * W_O\\nwhere head_i = Attention(Q*W_Q_i, K*W_K_i, V*W_V_i)\\n4. Why Attention Changed Everything\\nBefore attention, sequence-to-sequence models used RNNs and LSTMs which suffered from the\\nvanishing gradient problem and struggled to capture long-range dependencies. Attention solved this by\\nproviding direct connections between any two positions in the sequence, regardless of distance. This\\nmade training faster, more parallelizable (unlike RNNs which are sequential), and more interpretable\\nsince we can visualize which tokens the model attends to.\\n5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field\\nAny token can directly attend to any other token\\nInterpretability\\nAttention weights provide insight into model decisions\\nPermutation Equivariant\\nNo built-in notion of order (positional encoding needed)\\nQuadratic Complexity\\nO(n^2) cost w.r.t. sequence length — a known limitation\\n6. Summary\\nAttention mechanisms revolutionized deep learning by enabling models to dynamically focus on\\nrelevant information. From the original Bahdanau attention to modern multi-head self-attention, these\\ntechniques power state-of-the-art NLP, vision, and multimodal systems. Understanding attention is'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='essential for anyone working with Transformers, BERT, GPT, or any modern AI system.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"Transformer Architecture & Self-Attention\\nDeep Dive into 'Attention is All You Need'\\n1. The Transformer Model\\nThe Transformer, introduced by Vaswani et al. in 2017 in the landmark paper 'Attention is All You\\nNeed', completely replaced recurrence and convolutions with attention mechanisms. It consists of an\\nencoder stack and a decoder stack, each made up of identical layers. This architecture became the\\nfoundation for BERT, GPT, T5, and virtually every state-of-the-art NLP model today.\\n2. Encoder Architecture\\nThe encoder is composed of N identical layers (typically N=6). Each layer has two sub-layers: a\\nmulti-head self-attention mechanism, and a position-wise fully connected feed-forward network. Each\\nsub-layer has a residual connection followed by layer normalization.\\nLayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\\npassing through the attention or FFN transformations, making it much easier to train deep networks.\\n3. Positional Encoding\\nSince self-attention has no inherent notion of order (it's permutation equivariant), positional encodings\\nare added to the input embeddings to inject information about token positions. The original Transformer\\nused sinusoidal positional encodings:\\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\nSinusoidal encodings have the nice property that the encoding for position pos+k can be expressed as\\na linear function of the encoding at pos, which may help the model generalize to unseen sequence\\nlengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\\nHere's exactly how scaled dot-product attention works:\\nStep\\nOperation\\nShape\\n1\\nProject input X into Q, K, V matrices\\n(seq_len, d_model) → (seq_len, d_k)\\n2\\nCompute dot product: Q × K^T\\n(seq_len, seq_len)\\n3\\nScale by 1/sqrt(d_k)\\n(seq_len, seq_len)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"4\\nApply optional mask (for decoder)\\n(seq_len, seq_len)\\n5\\nApply softmax row-wise\\n(seq_len, seq_len) — rows sum to 1\\n6\\nMultiply by V\\n(seq_len, d_v)\\n5. The Decoder and Masked Attention\\nThe decoder has three sub-layers per block: a masked multi-head self-attention layer, a cross-attention\\nlayer (attending to the encoder output), and a feed-forward network. The masking in the first sub-layer\\nprevents the decoder from attending to future positions — this is critical for autoregressive generation\\nwhere the model must predict one token at a time without 'cheating' by looking at future tokens.\\n6. Feed-Forward Networks in Transformers\\nEach Transformer layer contains a position-wise FFN applied independently to each position:\\nFFN(x) = max(0, x*W_1 + b_1) * W_2 + b_2\\nTypically the inner dimension d_ff = 4 * d_model (e.g., 2048 when d_model=512). This FFN is where\\nmuch of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\\n7. Transformer Hyperparameters (Original Paper)\\nHyperparameter\\nBase Model\\nLarge Model\\nd_model (embedding dim)\\n512\\n1024\\nNumber of layers (N)\\n6\\n6\\nNumber of heads (h)\\n8\\n16\\nd_k = d_v\\n64\\n64\\nd_ff (FFN inner dim)\\n2048\\n4096\\nDropout rate\\n0.1\\n0.3\\nParameters\\n65M\\n213M\\n8. Why Transformers Beat RNNs\\nRNNs process sequences token by token, making parallelization impossible during training. LSTMs\\nmitigate vanishing gradients but still struggle with very long sequences. Transformers process all\\ntokens simultaneously, support full parallelism on GPUs/TPUs, handle long-range dependencies with\\nO(1) path length between any two positions, and scale remarkably well with data and compute —\\nleading to the era of large language models.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)\\nThis allows the attention computation to be rewritten using matrix associativity, computing KV\\naggregations first and then applying Q — avoiding the n×n attention matrix entirely. Examples include\\nPerformer (using random Fourier features) and Linear Transformer.\\n4. Flash Attention'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Flash Attention (Dao et al., 2022) takes a different approach — instead of approximating attention, it\\ncomputes exact attention but uses a hardware-aware algorithm to minimize memory I/O. The key\\ninsight is that the bottleneck in standard attention is not compute but memory bandwidth between HBM\\n(high-bandwidth memory) and SRAM (fast on-chip memory).\\nFlash Attention uses tiling to split Q, K, V into blocks and processes them in SRAM, never materializing\\nthe full n×n attention matrix in HBM. This achieves 2-4x speedup and O(n) memory usage while\\ncomputing mathematically identical results to standard attention. Flash Attention 2 and 3 further\\nimproved performance with better parallelism strategies.\\n5. Multi-Query and Grouped-Query Attention\\n5.1 Multi-Query Attention (MQA)\\nMulti-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —\\ncritical for autoregressive generation where cached keys and values consume large amounts of\\nmemory. MQA is used in models like PaLM and Falcon.\\n5.2 Grouped-Query Attention (GQA)\\nGrouped-Query Attention (Ainslie et al., 2023) is a middle ground between MHA and MQA. Query\\nheads are divided into G groups, and each group shares one key and value head. GQA achieves\\nquality close to MHA with inference speed close to MQA. It is used in Llama 2, Llama 3, Mistral, and\\nmany modern open-source LLMs.\\n6. Relative Positional Encodings\\n6.1 Rotary Position Embedding (RoPE)\\nRoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance\\n(m-n), giving the model a natural sense of relative position. RoPE generalizes well to longer sequences\\nthan seen during training and is used in GPT-NeoX, LLaMA, Falcon, and most modern LLMs.\\n6.2 ALiBi (Attention with Linear Biases)\\nALiBi (Press et al., 2021) adds a linear bias to attention scores based on the distance between query\\nand key positions: score(q_i, k_j) -= m * |i - j|, where m is a head-specific slope. This penalizes\\nattending to distant tokens, encouraging locality. ALiBi models extrapolate well to longer sequences\\nwithout any retraining.\\n7. Comparison of Attention Variants'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Method\\nComplexity\\nExact?\\nUse Case\\nStandard MHA\\nO(n^2)\\nYes\\nGeneral purpose, short sequences\\nLongformer\\nO(n * w)\\nApprox\\nLong documents\\nBigBird\\nO(n)\\nApprox\\nVery long sequences\\nFlash Attention\\nO(n^2) compute, O(n) mem\\nYes\\nGPU-efficient training\\nLinear Attention\\nO(n)\\nApprox\\nExtreme length, efficiency\\nMQA / GQA\\nO(n^2)\\nYes\\nFast inference, small KV cache\\n8. Summary & Takeaways\\nThe field of attention mechanisms has evolved rapidly since 2017. Standard multi-head attention\\nremains the gold standard for quality, while efficient variants like Flash Attention, GQA, and sparse\\nattention methods make it practical to scale to longer sequences and larger models. Choosing the right\\nattention variant depends on your sequence length, hardware constraints, inference requirements, and\\nacceptable quality trade-offs. Modern LLMs like Llama 3 and Mistral combine Flash Attention + GQA +\\nRoPE for an optimal balance of quality, speed, and memory efficiency.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "438eb97d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'split_docs' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmetadata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_docs[\u001b[32m0\u001b[39m].metadata\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m split_docs\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m chunks=\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_pdf_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m chunks\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36msplit_documents\u001b[39m\u001b[34m(documents, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m      8\u001b[39m text_splitter=RecursiveCharacterTextSplitter(\n\u001b[32m      9\u001b[39m     chunk_size=chunk_size,\n\u001b[32m     10\u001b[39m     chunk_overlap=chunk_overlap,\n\u001b[32m     11\u001b[39m     length_function=\u001b[38;5;28mlen\u001b[39m, \u001b[38;5;66;03m# this is the function that will be used to calculate the length of the text, in this case we are using the built-in len function which counts the number of characters in the text. This is important because we want to split the text into chunks of a certain size, and we need to know how long the text is to do that.\u001b[39;00m\n\u001b[32m     12\u001b[39m     separators=[\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m#  these are the separators that will be used to split the text. The text splitter will try to split the text using these separators in order. So it will first try to split the text using double newlines, then single newlines, then spaces, and finally if it can't split the text using any of those separators, it will split the text at the chunk size regardless of the separator. This is important because we want to try to split the text at natural break points (like paragraphs or sentences) before splitting it at arbitrary points (like in the middle of a word).\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#add chunk metadata + stable chunk_id based on content hash\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i,d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43msplit_docs\u001b[49m):\n\u001b[32m     17\u001b[39m     d.metadata[\u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m]=i\n\u001b[32m     19\u001b[39m     source=d.metadata.get(\u001b[33m\"\u001b[39m\u001b[33msource_path\u001b[39m\u001b[33m\"\u001b[39m,d.metadata.get(\u001b[33m\"\u001b[39m\u001b[33msource_file\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33munknown_source\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'split_docs' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "### text splitting get into chunks\n",
    "\n",
    "from langchain_core import documents\n",
    "import hashlib # we will use this to create a unique hash for each chunk based on its content, this will help us to avoid storing duplicate chunks in the vector store and also to easily identify which chunks are similar when we retrieve them later.\n",
    "\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200): # what is a chunk overlap - ans - it is the number of characters that will be repeated in the next chunk to provide context\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len, # this is the function that will be used to calculate the length of the text, in this case we are using the built-in len function which counts the number of characters in the text. This is important because we want to split the text into chunks of a certain size, and we need to know how long the text is to do that.\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"] #  these are the separators that will be used to split the text. The text splitter will try to split the text using these separators in order. So it will first try to split the text using double newlines, then single newlines, then spaces, and finally if it can't split the text using any of those separators, it will split the text at the chunk size regardless of the separator. This is important because we want to try to split the text at natural break points (like paragraphs or sentences) before splitting it at arbitrary points (like in the middle of a word).\n",
    "    )\n",
    "\n",
    "    #add chunk metadata + stable chunk_id based on content hash\n",
    "    for i,d in enumerate(split_docs):\n",
    "        d.metadata[\"chunk_index\"]=i\n",
    "\n",
    "        source=d.metadata.get(\"source_path\",d.metadata.get(\"source_file\",\"unknown_source\"))\n",
    "        page=d.metadata.get(\"page\",\"unknown_page\")\n",
    "\n",
    "        #stable chunk id\n",
    "        raw=f\"{source}|{page}|{i}|{d.page_content}\".encode(\"utf-8\")\n",
    "        d.metadata[\"chunk_id\"]=hashlib.sha1(raw).hexdigest() # we use sha1 here because it is fast and produces a short hash, we don't need a cryptographic hash for this purpose\n",
    "        \n",
    "\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(\"\\n example chunk:\")\n",
    "        print(f\"content: {split_docs[0].page_content[:200]}\")\n",
    "        print(f\"metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "\n",
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bdf5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb3bfbc",
   "metadata": {},
   "source": [
    "### Embedding & Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2d084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec499994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "from typing  import List , Dict , Any , Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebf15bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model:all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38263b3441545548315a4d05c0221dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension:384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2340f67ba10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentance embeddings\n",
    "        \"\"\"\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model() # its a protected function because we don't want the user to call it directly, we want them to use the get_embedding function which will call this function if the model is not already loaded.\n",
    "\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the SentenceTransformer model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model:{self.model_name}\")\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension:{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading model {self.model_name}:{e}\")\n",
    "            raise e\n",
    "        \n",
    "    def generate_embeddings(self,texts:List[str])->np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "        texts:list of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddins with shape (len(texts),embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Call _load_model() to load the model before generating embeddings.\")\n",
    "        \n",
    "        print(f\"Generating embeddings fro {len(texts)} texts\")\n",
    "        embeddings=self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embedings with stage:{embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b4092",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7006c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store initialize. collection:pdf_documents\n",
      "Existing documents in collection:574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2340f7792b0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages a vector store using ChromaDB for storing and retrieving document embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self,collection_name:str =\"pdf_documents\",persist_directory: str =\"../data/vector_store\"):\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\n",
    "                    \"description\":\"PDF document embeddings for RAG\",\n",
    "                    \"hnsw:space\":\"cosine\" # this is a custom metadata field that we can use to specify the distance metric for the HNSW index. ChromaDB will use this information to optimize the index for cosine similarity, which is the most common distance metric for text embeddings. This is important because it will improve the performance of similarity search in the vector store.\n",
    "                }\n",
    "            )\n",
    "            print(f\"vector store initialize. collection:{self.collection_name}\")\n",
    "            print(f\"Existing documents in collection:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        \n",
    "        \"\"\"\n",
    "        add documents and their embedings tp the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: list of documents to add to the store. Each document should have a unique ID in its metadata under the key \"id\".\n",
    "            embeddings: numpy array of embeddings corresponding to the documents, with shape (len(documents), embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if(len(documents)!=len(embeddings)):\n",
    "            raise ValueError(\"number of documents and embeddings must be the same\" )\n",
    "\n",
    "        print(f\"adding {len(documents)} document to vector store...\")\n",
    "\n",
    "        #prepare data for chroma db    \n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_text=[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "\n",
    "            #generate id\n",
    "            doc_id=doc.metadata.get(\"chunk_id\") # generate a unique id for the document using uuid and the index of the document in the list. This ensures that even if there are duplicate documents, they will have unique IDs in the vector store.\n",
    "            if not doc_id:\n",
    "                doc_id=f\"fallback_{uuid.uuid4().hex}\"\n",
    "\n",
    "            #generate metadata\n",
    "            metadata=dict(doc.metadata) # make a copy of the document metadata to avoid modifying the original document's metadata\n",
    "            metadata['doc_index']=i # add the index of the document in the original list to the metadata. This can be useful for debugging and for retrieving the original document later if needed.\n",
    "            metadata['context_length']=len(doc.page_content) # add the length of the document text to the metadata. This can be useful for filtering documents based on their length during retrieval.  \n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #Document content\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #embeddings\n",
    "            embeddings_list.append(embedding.tolist()) # convert the embedding from a numpy array to a list so that it can be stored in ChromaDB, which expects embeddings to be in list format.\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"succesfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"total documnets in collection:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error adding documents to vector store: {e}\")\n",
    "            raise e \n",
    "        \n",
    "\n",
    "vector_store=VectorStore()\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0eb1625b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 0, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Deterministic\\nFinite Automata\\n9/18/2024\\n1\\nFinite Automata\\nAnd Regular Languages'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 1, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Finite Automata\\n• Languages are generally infinite, but must \\nbe describable in some finite way\\n• Finite Automata - recognizer\\nFinite Automata  recognizer\\n• Regular expression – notations to \\ndescribe how strings of the language can \\nbe generated.\\n2\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 2, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Deterministic Finite Automaton (DFA)\\nInput Tape\\nString\\nOutput\\n9/18/2024\\n3\\n“Accept”\\nor\\n“Reject”\\nFinite\\nAutomaton'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 3, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Finite Automata\\n• Finite automata are finite collections of states \\nwith transition rules that take you from one state \\nto another.\\n• Original application was sequential switching \\ncircuits where the “state” was the settings of\\ncircuits, where the state  was the settings of \\ninternal bits.\\n• Informally, a state diagram that comprehensively \\ncaptures all possible states and transitions that a \\nmachine can take while responding to a stream or \\nsequence of input symbols\\n• Recognizer for “Regular Languages”\\n4\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 4, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Finite Automaton (FA)\\nDeterministic Finite Automata (DFA)\\nThe machine can exist in only one state at \\nany given time\\n5\\nNon-deterministic Finite Automata (NFA)\\nThe machine can exist in multiple states at \\nthe same time\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 5, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Transition Graph\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\nb\\na,\\n9/18/2024\\n6\\ninitial\\nstate\\naccepting\\nstate\\nstate\\ntransition\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\na\\nb'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 6, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='5\\nq\\na\\na\\nb\\nb\\nb\\na,\\nb\\na,\\n}\\n,\\n{\\nb\\na\\n\\uf03d\\n\\uf053\\nAlphabet\\n9/18/2024\\n7\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\na\\nb\\nFor every state, there is a transition\\nfor every symbol in the alphabet'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 7, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Initial Configuration\\nb\\na,\\nInput String\\na b b a\\nInput Tape\\nhead\\n9/18/2024\\n8\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq\\nInitial state'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 8, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Scanning the Input\\nb\\na,\\na b b a\\n9/18/2024\\n9\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 9, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\na b b a\\n9/18/2024\\n10\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 10, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\na b b a\\n9/18/2024\\n11\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 11, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\na b b a\\nInput finished\\n9/18/2024\\n12\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\naccept\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 12, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\na b a\\nA Rejection Case\\nInput String\\n9/18/2024\\n13\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 13, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\na b a\\n9/18/2024\\n14\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 14, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\na b a\\n9/18/2024\\n15\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 15, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\nreject\\na b a\\nInput finished\\n9/18/2024\\n16\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nreject\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 16, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\n)\\n(\\uf06c\\nAnother Rejection Case\\nTape is empty\\nInput Finished\\n9/18/2024\\n17\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq\\nreject'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 17, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language Accepted:\\n\\uf07b\\n\\uf07d\\nabba\\nL \\uf03d\\nb\\na,\\n9/18/2024\\n18\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 18, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='To accept a string:\\nall the input string is scanned \\nand the last state is accepting\\n9/18/2024\\n19\\nTo reject a string:\\nall the input string is scanned \\nand the last state is non-accepting'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 19, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Another Example\\nb\\na,\\n\\uf07b\\n\\uf07d\\nabba\\nab\\nL\\n,\\n,\\n\\uf06c\\n\\uf03d\\n9/18/2024\\n20\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\nAccept\\nstate\\nAccept\\nstate\\nAccept\\nstate'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 20, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content=')\\n(\\uf06c\\nEmpty Tape\\nb\\na,\\nInput Finished\\n9/18/2024\\n21\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\naccept'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 21, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Another Example\\na\\nb\\na,\\n9/18/2024\\n22\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq\\nAccept \\nstate\\ntrap state \\nor dead state'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 22, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\na\\nInput String\\n9/18/2024\\n23\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 23, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\na\\n9/18/2024\\n24\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 24, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\na\\n9/18/2024\\n25\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 25, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\na\\nInput finished\\n9/18/2024\\n26\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq\\naccept'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 26, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\nb\\nA rejection case\\nInput String\\n9/18/2024\\n27\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 27, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\nb\\n9/18/2024\\n28\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 28, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\nb\\n9/18/2024\\n29\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 29, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='a\\nb\\na,\\na\\nb\\nb\\nInput finished\\n9/18/2024\\n30\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq\\nreject'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 30, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language Accepted:\\n}\\n0\\n:\\n{\\n\\uf0b3\\n\\uf03d\\nn\\nb\\na\\nL\\nn\\na\\nb\\na,\\n9/18/2024\\n31\\nb\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 31, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Another Example\\n0\\nq\\n1\\nq\\n1\\n}\\n1{\\n\\uf03d\\n\\uf053\\nAlphabet:\\n9/18/2024\\n32\\n0\\nq\\n1\\nq\\n1\\nLanguage Accepted:\\neven}\\nis\\nand\\n:\\n{\\n*\\nx\\nx\\nx\\nEVEN\\n\\uf053\\n\\uf0ce\\n\\uf03d\\n}\\n,\\n111111\\n,\\n1111\\n,\\n11\\n,\\n{\\n\\uf04b\\n\\uf06c\\n\\uf03d'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 32, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Formal Definition\\nDeterministic Finite Automaton (DFA)\\n\\uf028\\n\\uf029\\nF\\nq\\nQ\\nM\\n,\\n,\\n,\\n,\\n0\\n\\uf064\\n\\uf053\\n\\uf03d\\nQ\\n: set of states\\n9/18/2024\\n33\\n\\uf053\\n\\uf064\\n0\\nq\\nF\\n: input alphabet\\n: transition function\\n: initial state\\n: set of accepting states\\n\\uf053\\n\\uf0cf\\n\\uf06c\\nQ x ∑ ==> Q'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 33, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Set of States Q\\nb\\na,\\n\\uf07b\\n\\uf07d\\n5\\n4\\n3\\n2\\n1\\n0\\n,\\n,\\n,\\n,\\n,\\nq\\nq\\nq\\nq\\nq\\nq\\nQ \\uf03d\\nExample\\n9/18/2024\\n34\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 34, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Input Alphabet \\uf053\\nb\\na,\\n\\uf07b\\n\\uf07d\\nb\\na,\\n\\uf03d\\n\\uf053\\n:the input alphabet never contains \\uf06c\\n\\uf053\\n\\uf0cf\\n\\uf06c\\nExample\\n9/18/2024\\n35\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 35, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Initial State 0\\nq\\nb\\na,\\nExample\\n9/18/2024\\n36\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 36, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Set of Accepting States\\nQ\\nF \\uf0cd\\nb\\na,\\n\\uf07b\\uf07d\\n4\\nq\\nF \\uf03d\\nExample\\n9/18/2024\\n37\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n4\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 37, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Transition Function \\nQ\\nQ\\n\\uf0ae\\n\\uf053\\n\\uf0b4\\n:\\n\\uf064\\nq\\nq\\uf0a2\\nx\\nq\\nx\\nq\\n\\uf0a2\\n\\uf03d\\n)\\n,\\n(\\n\\uf064\\n9/18/2024\\n38\\nq\\nq\\nDescribes the result of a transition\\nfrom state       with symbol\\nq\\nx'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 38, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\n\\uf028\\n\\uf029\\n1\\n0,\\nq\\na\\nq\\n\\uf03d\\n\\uf064\\nExample:\\n9/18/2024\\n39\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq\\n1\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 39, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\n5\\n0,\\nq\\nb\\nq\\n\\uf03d\\n\\uf064\\nb\\na,\\n9/18/2024\\n40\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 40, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\n\\uf028\\n\\uf029\\n3\\n2,\\nq\\nb\\nq\\n\\uf03d\\n\\uf064\\n9/18/2024\\n41\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 41, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf064\\na\\nb\\n0\\nq\\n1\\nq\\n1\\nq\\n5\\nq\\n5\\nq\\n2\\nq\\nTransition Table for \\nsymbols\\n\\uf064\\n9/18/2024\\n42\\n2\\nq\\n3\\nq\\n4\\nq\\n5\\nq\\n5\\nq\\n3\\nq\\n4\\nq\\n5\\nq\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\nb\\na,\\n5\\nq\\n5\\nq\\n5\\nq\\n5\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 42, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='What does a DFA do on reading an input \\nstring?\\n• Input: a word w in ∑*\\n• Question: Is w acceptable by the DFA?\\n• Steps:\\n– Start at the “start state” q0\\n43\\n– For every input symbol in the sequence w do\\n• Compute the next state from the current \\nstate, given the current input symbol in w \\nand the transition function\\n– If after all symbols in w are consumed, the \\ncurrent state is one of the accepting states (F) \\nthen accept w;\\n– Otherwise, reject w.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 43, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example #1\\n• Build a DFA for the following language:\\n• L = {w | w is a binary string that contains 01 as \\na substring}\\n• Steps for building a DFA to recognize L:\\n• ∑= {0 1}\\n44\\n∑ = {0,1}\\n• Decide on the states: Q\\n• Designate start state and final state(s)\\n• δ: Decide on the transitions: \\n• “Final” states == same as “accepting states”\\n• Other states == same as “non-accepting states”\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 44, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='DFA for strings containing 01\\n1\\n0,1\\n0\\n• What makes this DFA deterministic?\\n• Q = {q0,q1,q2}\\n• ∑ = {0,1}\\n• start state = q0\\n45\\nq0\\nstart\\nq1\\n0\\n1\\nq2\\nAccepting\\nstate\\n• What if the language allows \\nempty strings?\\n• F = {q2} \\n• Transition table\\nq2\\nq2\\n*q2\\nq2\\nq1\\nq1\\nq0\\nq1\\nq0\\n1\\n0\\nstates\\nsymbols\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 45, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example #2\\nClamping Logic: \\n• A clamping circuit waits for a ”1” input, and turns on \\nforever. However, to avoid clamping on spurious noise, \\nwe’ll design a DFA that waits for two consecutive 1s in \\na row before clamping on.\\nB ild\\nDFA f\\nth\\nf ll\\ni\\nl\\n46\\n• Build a DFA for the following language:\\nL = { w | w is a bit string which contains the substring 11}\\n• State Design:\\n• q0 : start state (initially off), also means the most \\nrecent input was not a 1\\n• q1: has never seen 11 but the most recent input was a 1\\n• q2: has seen 11 at least once\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 46, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example #3\\nBuild a DFA for the following language:\\nL = { w | w is a binary string that has even \\nnumber of 1s and even number of 0s}\\n47\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 47, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example #4\\nL = { x ϵ {a,b}* / x ends with aa}\\n48\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 48, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Extended Transition Function (for acceptance of string)\\nQ\\nQ\\n\\uf0ae\\n\\uf053\\n\\uf0b4\\n*\\n* :\\n\\uf064\\nq\\nw\\nq\\n\\uf0a2\\n\\uf03d\\n)\\n,\\n(\\n*\\n\\uf064\\nQ\\nQ\\n\\uf0ae\\n\\uf053\\n\\uf0b4\\n:\\n\\uf064\\n9/18/2024\\n49\\nq\\nw\\nq\\n)\\n,\\n(\\n\\uf064\\nDescribes the resulting state \\nafter scanning string        from state\\nw\\nq\\n)\\n),\\n,\\n(\\n(\\n)\\n,\\n(\\n*\\n*\\na\\nw\\nq\\nwa\\nq\\n\\uf064\\n\\uf064\\n\\uf064\\n\\uf03d'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 49, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Extended Transition Function \\n9/18/2024\\n50'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 50, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\n2\\n0\\n*\\n,\\nq\\nab\\nq\\n\\uf03d\\n\\uf064\\nb\\na\\nExample:\\n9/18/2024\\n51\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\nb\\na,\\n0\\nq\\n1\\nq\\n2\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 51, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\n5\\n0\\n*\\n,\\nq\\nabbbaa\\nq\\n\\uf03d\\n\\uf064\\nb\\na,\\n9/18/2024\\n52\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,\\n0\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 52, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\n4\\n1\\n*\\n,\\nq\\nbba\\nq\\n\\uf03d\\n\\uf064\\nb\\na,\\n9/18/2024\\n53\\n0\\nq\\n1\\nq\\n2\\nq\\n3\\nq\\n4\\nq\\na\\nb\\nb\\na\\n5\\nq\\na\\na\\nb\\nb\\nb\\na,'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 53, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Special case:\\nfor any state q\\n9/18/2024\\n54\\n\\uf028\\n\\uf029\\nq\\nq\\n\\uf03d\\n\\uf06c\\n\\uf064\\n,\\n*'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 54, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\nq\\nw\\nq\\n\\uf0a2\\n\\uf03d\\n,\\n*\\n\\uf064\\n\\uf0a2\\nk\\nw\\n\\uf073\\n\\uf073\\n\\uf073\\n\\uf04c\\n2\\n1\\n\\uf03d\\n1\\n\\uf073\\n2\\n\\uf073\\nk\\n\\uf073\\nIn general:\\nimplies that there is a walk of transitions\\n9/18/2024\\n55\\nq\\nq\\uf0a2\\nw\\nq\\nq\\uf0a2\\n1\\n\\uf073\\n2\\n\\uf073\\nk\\n\\uf073\\nstates may be repeated'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 55, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language ofDFA\\nit is denoted as           and contains\\nall the strings accepted by  \\n\\uf028\\n\\uf029\\nM\\nL\\nM\\nM\\nLanguage of DFA       :\\n9/18/2024\\n56\\nWe say that a language       \\nis accepted (or recognized) \\nby DFA          if \\nM\\nL\\uf0a2\\n\\uf028\\n\\uf029\\nL\\nM\\nL\\n\\uf0a2\\n\\uf03d'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 56, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language accepted by DFA\\nA DFA M accepts string w if there is a path \\nfrom q0 to an accepting (or final) state \\nthat is labeled by w\\nI\\nL(M)\\nll t i\\nth t l\\nd t\\n57\\nI.e., L(M) = all strings that lead to an \\naccepting state from q0\\n9/18/2024'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 57, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='For a DFA\\nLanguage accepted by       :\\n\\uf028\\n\\uf029\\nF\\nq\\nQ\\nM\\n,\\n,\\n,\\n,\\n0\\n\\uf064\\n\\uf053\\n\\uf03d\\nM\\n\\uf028\\n\\uf029\\n\\uf028\\n\\uf029\\n\\uf07b\\n\\uf07d\\nF\\nM\\nL\\n\\uf053:\\n*\\n* \\uf064\\n9/18/2024\\n58\\n\\uf028\\n\\uf029\\n\\uf028\\n\\uf029\\n\\uf07b\\n\\uf07d\\nF\\nw\\nq\\nw\\nM\\nL\\n\\uf0ce\\n\\uf053\\n\\uf0ce\\n\\uf03d\\n,\\n:\\n0\\n\\uf064\\n0\\nq\\nq\\uf0a2\\nw\\nF\\nq \\uf0ce\\n\\uf0a2'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 58, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language rejected by      :\\n\\uf028\\n\\uf029\\n\\uf028\\n\\uf029\\n\\uf07b\\n\\uf07d\\nF\\nw\\nq\\nw\\nM\\nL\\n\\uf0cf\\n\\uf053\\n\\uf0ce\\n\\uf03d\\n,\\n:\\n0\\n*\\n* \\uf064\\nM\\n9/18/2024\\n59\\n0\\nq\\nq\\uf0a2\\nw\\nF\\nq \\uf0cf\\n\\uf0a2'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 59, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='More DFA Examples\\nb\\na,\\n}\\n,\\n{\\nb\\na\\n\\uf03d\\n\\uf053\\n*\\n)\\n(\\n\\uf053\\n\\uf03d\\nM\\nL\\nb\\na,\\n}\\n{\\n)\\n(\\n\\uf03d\\nM\\nL\\n9/18/2024\\n60\\n0\\nq\\n0\\nq\\n,\\nEmpty language\\nAll strings'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 60, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='b\\na,\\n}\\n,\\n{\\nb\\na\\n\\uf03d\\n\\uf053\\nLanguage of the empty string\\n9/18/2024\\n61\\n0\\nq\\n0\\nq\\nb\\na,\\n}\\n{\\n)\\n(\\n\\uf06c\\n\\uf03d\\nM\\nL'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 61, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\nM\\nL\\n= { all strings with prefix       }\\nab\\nb\\na,\\n}\\n,\\n{\\nb\\na\\n\\uf03d\\n\\uf053\\n9/18/2024\\n62\\na\\nb\\n0\\nq\\n1\\nq\\n2\\nq\\naccept\\nb\\na,\\n3\\nq\\na\\nb'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 62, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\nM\\nL\\n= { all binary strings containing\\nsubstring          }\\n001\\n1\\n0\\n1,\\n0\\n9/18/2024\\n63\\n\\uf06c\\n0\\n00\\n001\\n1\\n0\\n1\\n1\\n0\\n0'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 63, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf028\\n\\uf029\\nM\\nL\\n= { all binary strings without\\nsubstring          }\\n001\\n1\\n0\\n1\\n0\\n9/18/2024\\n64\\n\\uf06c\\n0\\n00\\n001\\n0\\n1\\n1\\n0\\n1,\\n0'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 64, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf07b\\n\\uf07d\\n\\uf07b\\n\\uf07d\\n*\\n,\\n:\\n)\\n(\\nb\\na\\nw\\nawa\\nM\\nL\\n\\uf0ce\\n\\uf03d\\nb\\nb\\na\\n9/18/2024\\n65\\na\\nb\\nb\\na,\\na\\n0\\nq\\n2\\nq\\n3\\nq\\n4\\nq'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 65, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Regular Languages\\nDefinition:\\nA language      is regular if there is \\na DFA        that accepts it (              )\\nL\\nM\\nL\\nM\\nL\\n\\uf03d\\n)\\n(\\n9/18/2024\\n66\\nThe languages accepted by all DFAs \\nform the family of regular languages'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 66, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf07b\\n\\uf07d\\nabba\\n\\uf07b\\n\\uf07d\\nabba\\nab,\\n,\\n\\uf06c\\n}\\n0\\n:\\n{\\n\\uf0b3\\nn\\nb\\nan\\n{ all strings in {a,b}* with prefix       }\\nab\\nExample regular languages:\\n\\uf07b\\n\\uf07d\\n\\uf07b\\n\\uf07d\\n*\\n,\\n:\\nb\\na\\nw\\nawa\\n\\uf0ce\\n9/18/2024\\n67\\n{ all binary strings without substring        }\\n001\\nThere exist automata that accept these\\nlanguages (see previous slides).\\neven}\\nis\\nand\\n}\\n1{\\n:\\n{\\n*\\nx\\nx\\nx\\n\\uf0ce\\n}\\n{\\n}\\n{\\uf06c\\n*}\\n,\\n{\\nb\\na'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 67, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Draw DFA for the following regular languages:\\n• FA accepting Binary representation of integer\\nDivisible by 3\\n• FA accepting Binary representation of integer\\nDivisible by 4\\n• FA accepting all string w over {a,b} that begins and ends       \\nwith the same symbol\\nFall 2006\\n68\\nwith the same symbol\\n• Draw an FA having Na(w)   0 mod 3\\ni.e. number of a in string is divisible by 3\\n• L={ w | |w|=3} \\n• FA accepting binary strings that begins and ends with   \\ndifferent symbols\\n\\uf040'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 68, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n•FA accepting Binary representation \\nof integer Divisible by 4\\n9/18/2024\\n69'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 69, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='There exist languages which are not Regular:\\n}\\n0\\n:\\n{\\n\\uf0b3\\n\\uf03d\\nn\\nb\\na\\nL\\nn\\nn\\nLanguage of even/odd length palindrome\\n9/18/2024\\n70\\nThere is no DFA that accepts these languages\\n(we will prove this in a later class)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 70, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Closure Properties of Regular \\nLanguages\\nIf we have a finite automata accepting the \\nLanguage L1 and L2 then there is a FA \\naccepting\\nL1\\nL2\\n9/18/2024\\n71\\nL1 \\uf0c8L2\\nL1 Ո L2\\nL1 – L2\\nComplement of L'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 71, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Union of Two Languages\\nTheorem 1.12: If L1 and L2 are regular \\nlanguages, then so is L1 \\uf0c8L2.\\n(The regular languages are ‘closed’ under\\nthe union operation.)\\nthe union operation.)\\nProof idea: L1 and L2 are regular, hence \\nthere are two DFA M1 and M2, with L1=L(M1) \\nand L2=L(M2).\\nOut of these two DFA, we will make a third \\nautomaton M3 such that L(M3) = L1 \\uf0c8L2.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 72, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof Union-Theorem (1)\\nM1=(Q1,\\uf053,\\uf0641,q1,F1) and M2=(Q2,\\uf053,\\uf0642,q2,F2)\\nDefine M3 = (Q3,\\uf053,\\uf0643,q3,F3) by:\\n• Q3 = Q1\\uf0b4Q2 = {(r1,r2) | r1\\uf0ceQ1 and r2\\uf0ceQ2}\\nThink of running both the m/c parallelly:\\n• \\uf0643((r1,r2),a) = (\\uf0641(r1,a), \\uf0642(r2,a))\\n• q3 = (q1,q2)\\n•If  F3 = {(r1,r2) | r1\\uf0ceF1 or r2\\uf0ceF2}, M3 accept L1 \\uf0c8L2.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 73, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof Union-Theorem (2)\\n\\uf0643((r1,r2),a) = (\\uf0641(r1,a), \\uf0642(r2,a))\\nThis will follow immediately from the formula\\n\\uf064*(q3,x) = (\\uf0641*(q1,x), \\uf0642*(q2,x)) for every x \\uf0ce\\uf053*\\nq3\\n1\\nq1\\n2\\nq2\\ny\\nFor every string x, x is accepted by M3 if \\n\\uf064*(q3,x) \\uf0ceF3 \\nAnd according to definition of F3 and formula \\nfor \\uf064* this is true for every string x'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 74, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof Union-Theorem (2)\\nThe automaton M3 = (Q3,\\uf053,\\uf0643,q3,F3) runs M1 and \\nM2 in ‘parallel’ on a string w.\\nIn the end, the final state (r1,r2) ‘knows’\\nif w\\uf0ceL1 (via r1\\uf0ceF1?) and if w\\uf0ceL2 (via r2\\uf0ceF2?)\\nf\\n1 (\\n1\\n1 )\\nf\\n2 (\\n2\\n2 )\\nThe accepting states F3 of M3 are such that\\nw\\uf0ceL(M3) if and only if w\\uf0ceL1 or w\\uf0ceL2, for:\\nF3 = {(r1,r2) | r1\\uf0ceF1 or r2\\uf0ceF2}.\\nProof: i) if w\\uf0ceL(M3) ═> w\\uf0ceL1 or w\\uf0ceL2\\nii)  if w\\uf0ceL1 or w\\uf0ceL2 ═> w\\uf0ceL(M3)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 75, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Intersection of L1 and L2\\nDefinition: L1Ո L2\\nTheorem 1.13: If L1 and L2 are regular \\nlanguages, then so is L1Ո L2.\\n(The regular languages are ‘closed’ under\\nconcatenation.)\\nF3 = {(r1,r2) | r1\\uf0ceF1 and r2\\uf0ceF2}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 76, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example - Difference of L1 and L2\\nDefinition: L1- L2\\nTheorem 1.13: If L1 and L2 are regular \\nlangues, then so is L1- L2.\\n(The regular languages are ‘closed’ under\\n(The regular languages are closed  under\\ndifference.)\\nF3 = {(r1,r2) | r1\\uf0ceF1 and r2\\uf0cfF2}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 77, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example - Union\\n9/18/2024\\n78'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 78, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example - Union\\n9/18/2024\\n79'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 79, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example -Intersection\\n9/18/2024\\n80'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 80, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example - Differene\\n9/18/2024\\n81'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 81, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n• L1= {x\\uf0ce{a,b}*| aa is not a substring of x}\\n• L2 = {x\\uf0ce{a,b}*|x ends with ab}\\n9/18/2024\\n82'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 82, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n9/18/2024\\n83'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 83, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n9/18/2024\\n84\\nL1 \\uf0c8L2 \\nL1 Ո L2'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 84, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Distinguishing One String from Another\\nEg. FA M accepting L of string in {a,b}* \\nending with aa\\n1)˄ and a\\n85\\n1)˄ and a\\n2)˄ and aa\\n3)a and aa\\nCould FA be constructed with fewer than 3 states?   Or\\nCan we be sure that 3 states are enough?'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 85, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Distinguishing One String from Another\\nDefinition: \\n• Let L be a language in \\uf053*\\n• Two strings x and y in \\uf053* are \\ndistinguishable with respect to L if there \\n86\\ng\\np\\nis a string z∈\\uf053* (which may depend on x \\nand y) so that exactly one of the strings xz \\nand yz is in L\\n• The string z is said to distinguish x and y \\nwith respect to L'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 86, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Distinguishing One String from Another\\n• We may say that x and y are indistinguishable \\nwith respect to L if there is no such string z; \\n• In other words, if for every z, both xz and yz \\nhave the same status---either both in L or both \\nnot in L\\n87\\nEg:    L={x∈{0,1}* | x ends with 10}\\n• The strings 01011 and 100 are distinguishable \\nwith respect to L because for z=0, 01011z∈L \\nand 100z∉L. \\n• The strings 0 and 100 are indistinguishable with \\nrespect to L'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 87, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Distinguishing One String from Another\\n• Lemma \\nSuppose that L⊆\\uf053* and M=(Q, \\uf053, q0, F, \\uf064) is \\nany FA recognizing L. \\nIf x and y are two strings in \\uf053* for which\\n88\\nIf x and y are two strings in \\uf053for which \\n\\uf064*(q0,x)=\\uf064*(q0,y), then x and y are \\nindistinguishable with respect to L'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 88, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof:\\nLet z be any string in \\uf053* , and consider the \\ntwo strings xz and yz.\\n\\uf064*(q0,xz)= \\uf064*(\\uf064*(q0,x),z)\\n\\uf064*(q0,yz)= \\uf064*(\\uf064*(q0,y),z) \\nand therefore, by our assumption, \\n,\\ny\\np\\n,\\n\\uf064*(q0,xz)=\\uf064*(q0,yz). \\nSince M is assumed to recognize L, these \\ntwo strings are either both in L or both not \\nin L. Therefore, x and y are \\nindistinguishable with respect to L. ■'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 89, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Theorem\\nSuppose that L⊆\\uf053* and, for some positive \\ninteger n, there are n strings in \\uf053* , any two \\nof which are distinguishable with respect to \\nL. Then there can be no FA recognizing L with \\nDistinguishing One String from Another\\n90\\nfewer than n states.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 90, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='• L = {w | w is a binary string s.t., the nth symbol \\nfrom its right end is a 1}\\n– NFA has n+1 states\\nP.T a machine recognizing a string having 1 in the nth position \\nfrom right have atleast 2n states\\n91\\n– But an equivalent DFA needs to have at least 2n states\\n• Proof (By contradiction)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 91, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof\\n(Pigeon hole principle)\\n– m holes and pigeons >m\\n• => at least one hole has to contain two or more pigeons\\ne.g. let n=3, and M accepts language L and has less than 8 \\nstates\\n– No. of strings = 8 and No. of states are 7 or less\\nNo. of strings  8 and No. of states are 7 or less\\n– i.e. two strings takes machine to same state\\n• E.g. 010       but 010 ∈L and 110 does not ∈ L\\n110\\n• E.g. 100    considering substring 0, 1000 does not ∈ L\\n111\\n, 1110 ∈L\\n9/18/2024\\n92'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 92, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof\\n• For each n, the no. of stings of length \\nexactly n is 2n.\\n• So if we add these no. for the values of i \\nfrom 0 to n, we obtain total \\n2n+1-1 states. \\n• So, a machine recognizing a string having 1 \\nin the nth position from right have atleast \\n2n states\\n9/18/2024\\n93'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 93, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='9/18/2024\\n94'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 94, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Non-deterministic Finite Automata \\n(NFA)\\n• A Non-deterministic Finite Automaton \\n(NFA) \\n– is of course “non-deterministic”\\n• Implying that the machine can exist in more than \\n95\\np y\\none state at the same time\\n• Transitions could be non-deterministic (0,1, or more \\ntransition for an input symbol)\\nqi\\n1\\n1\\nqj\\nqk\\n…\\n• Each transition function therefore \\nmaps to a set of states'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 95, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Non-deterministic Finite Automata \\n(NFA)\\n• An NFA is also defined by the 5-tuple: \\n– {Q, ∑ , q0,F, δ }\\n• A Non-deterministic Finite Automaton (NFA)\\nconsists of:\\n96\\n– Q ==> a finite set of states\\n– ∑ ==> a finite set of input symbols (alphabet)\\n– q0 ==> a start state\\n– F ==> set of accepting states \\n– δ ==> a transition function, which is a mapping between \\nQ x ∑ ==> subset of Q  (2Q)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 96, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='NFA for strings containing 01\\n0 1\\n0,1\\n• Q = {q0,q1,q2}\\n• \\uf053= {0,1}\\nWhy is this non-deterministic? \\nRegular expression: (0+1)*01(0+1)*\\n97\\nq0\\nstart\\nq1\\n0\\n0,1\\n0,1\\n1\\nq2\\nFinal\\nstate\\n{ , }\\n• start state = q0\\n• F = {q2} \\n• Transition table\\n{q2}\\n{q2}\\n*q2\\n{q2}\\nΦ\\nq1\\n{q0}\\n{q0,q1}\\nq0\\n1\\n0\\nstates\\nsymbols\\nWhat will happen if at state q1\\nan input of 0 is received?'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 97, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='How to use an NFA?\\n• Input: a word w in ∑*\\n• Question: Is w acceptable by the NFA?\\n• Steps:\\n– Start at the “start state” q0\\n– For every input symbol in the sequence w do\\nD t\\ni\\nll\\nibl\\nt t t\\nf\\nll\\nt\\n98\\n• Determine all possible next states from all current \\nstates, given the current input symbol in w and the \\ntransition function\\n– If after all symbols in w are consumed and if at least \\none of the current states is a final state then accept \\nw;\\n– Otherwise, reject w.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 98, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\nInput: 0110\\n9/18/2024\\n99'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 99, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='What is an “error state”?\\n• A DFA for recognizing the key word “while”\\nq0\\nw\\nq1\\nh\\nq2\\ni\\nq3\\nl\\nq4\\ne\\nq5\\nNote: Omitting to explicitly show error states is just a matter of design convenience\\n(one that is generally followed for NFAs), and \\ni.e., this feature should not be confused with the notion of non-determinism. \\nWhat is an “error state”?\\n100\\n• An NFA for the same purpose:\\nqerr\\nAny other input symbol\\nq0\\nw\\nq1\\nh\\nq2\\ni\\nq3\\nl\\nq4\\ne\\nq5\\nAny symbol\\nTransitions into a dead state are implicit'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 100, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example #2\\n• Build an NFA for the following language:\\nL = { w | w ends in ab}\\n• Other examples\\n– Keyword recognizer (e.g., if, then, else, while, \\nfor include etc )\\n101\\nfor, include, etc.)\\n– L = { w | w starts with ab}\\n– L = { w | w consists 10 }\\n– L = {aa,aab}*{b}\\n– L = {w | w is a binary string s.t., the 4th\\nsymbol from its right end is a 0}\\n– L = {w|either 11 or 010 is a substr of w}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 101, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Examples\\nL = { w | w ends in ab}\\nL = { w | w consists 10 }\\n9/18/2024\\n102\\nL = {w | w is a binary string s.t., the 4th\\nsymbol from its right end is a 0}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 102, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Examples\\nDFA for L = {aa,aab}*{b}\\nNFA for L = {aa,aab}*{b}\\n9/18/2024\\n103'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 103, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Extension of δ to NFA Paths\\n• Let M ={Q, ∑ , q0,F, δ } be an NFA\\n• The function δ* : Q x ∑* ==> 2Q is defined as\\n• Basis: for any q ∈Q, δ*(q,\\uf065) = {q}\\nI d\\ni\\n104\\n• Induction:\\n– Let\\nδ*(q0,w) = {p1,p2…,pk}\\n– δ (pi,a) = Si for i=1,2...,k\\n– Then,   δ*(q0,wa) = S1 U S2 U … U Sk\\nor \\nδ*(q0,wa) = U δ(s,a)\\ns ∈ δ*(q0,w)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 104, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language of an NFA\\n• An NFA accepts w if there exists at least \\none path from the start state to an \\naccepting (or final) state that is labeled \\nby w\\n• L(N) = { w | δ*(q0 w) ∩F ≠Φ }\\n105\\nL(N)  { w | δ (q0,w) ∩F ≠ Φ }\\nWe say that a language L is accepted (or \\nrecognized) by NFA L(N) if L=L(N)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 105, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Input string: aaaabaab\\nWhat is the language accepted by NFA?\\n9/18/2024\\n106\\nL(M)={aa,aab}*{b}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 106, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='What is the language accepted by NFA?\\n9/18/2024\\n107\\nL(M)={x ∈{0,1}*|x has 4th symbol from \\nthe right end is 1}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 107, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Differences: DFA vs. NFA\\n•\\nDFA\\n1.\\nAll transitions are deterministic\\nEach transition leads to exactly one \\nstate\\n2.\\nFor each state, transition on all \\npossible symbols (alphabet) \\nshould be defined\\n•\\nNFA\\n1.\\nSome transitions could be non-\\ndeterministic\\nA transition could lead to a subset of \\nstates\\n2.\\nNot all symbol transitions need to \\nbe defined explicitly (if \\nundefined will go to an error \\nstate\\nthis is just a design\\n108\\n3.\\nAccepts input if the last state \\nvisited is in F\\n4.\\nSometimes harder to construct \\nbecause of the number of states\\n5.\\nPractical implementation is \\nfeasible\\nstate – this is just a design \\nconvenience, not to be confused \\nwith “non-determinism”)\\n3.\\nAccepts input if one of the last \\nstates is in F\\n4.\\nGenerally easier than a DFA to \\nconstruct\\n5.\\nPractical implementations limited \\nbut emerging\\nBut, DFAs and NFAs are equivalent in their power to capture langauges !!'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 108, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Equivalence of DFA & NFA\\n•\\nTheorem:\\n–\\nA language L is accepted by a DFA if and only if it is \\naccepted by an NFA.\\n•\\nProof:\\n1.\\nIf part:\\n•\\nProve by showing every NFA can be converted to an \\nequivalent DFA (in the next few slides…)\\nShould be \\ntrue for \\nany L\\n109\\nq\\nDF\\n(\\nf\\n)\\n2.\\nOnly-if part is trivial:\\n•\\nEvery DFA is a special case of an NFA where each state has \\nexactly one transition for every input symbol. Therefore, if L \\nis accepted by a DFA, it is accepted by a corresponding NFA.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 109, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Proof for the if-part\\n• If-part: A language L is accepted by a DFA if it is \\naccepted by an NFA\\n• rephrasing…\\n• Given any NFA N, we can construct a DFA D such \\nthat L(N)=L(D)\\n110\\nthat L(N) L(D)\\n• How to convert an NFA into a DFA?\\n– Observation: In an NFA, each transition maps to a \\nsubset of states \\n– Idea: Represent:\\neach “subset of NFA_states” \\uf0e8a single “DFA_state”\\nSubset construction'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 110, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='NFA to DFA by subset construction\\n•\\nLet N = {QN,∑,δN,q0,FN}\\n•\\nGoal: Build D={QD,∑,δD,{q0},FD} s.t. L(D)=L(N)\\n•\\nConstruction:\\n111\\nConstruction:\\n1. QD= the set of states of DFA\\n= all subsets of QN (i.e., power set)\\n1. FD=set of subsets S of QN s.t. S∩FN≠Φ\\n2. δD: for each subset S of QN and for each input \\nsymbol a in ∑: \\n•\\nδD(S,a) = U δN(p,a)\\np in s'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 111, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='NFA to DFA construction: Example\\n•\\nL = {w | w ends in 01}\\nq\\nq\\n0\\n0,1\\nq\\n1\\nNFA:\\nDFA:\\n[q0]\\n1\\n0\\n[q0,q1]\\n1\\n[q0,q2]\\n0\\n0\\n1\\nIdea: To avoid enumerating all of \\npower set, do \\n“lazy creation of states”\\n112\\nq0\\nq1\\nq2\\nδN\\n0\\n1\\nq0\\n{q0,q1}\\n{q0}\\nq1\\nØ\\n{q2}\\n*q2\\nØ\\nØ\\nδD\\n0\\n1\\nØ\\nØ\\nØ\\n[q0]\\n{q0,q1}\\n{q0}\\n[q1]\\nØ\\n{q2}\\n*[q2]\\nØ\\nØ\\n[q0,q1]\\n{q0,q1}\\n{q0,q2}\\n*[q0,q2]\\n{q0,q1}\\n{q0}\\n*[q1,q2]\\nØ\\n{q2}\\n*[q0,q1,q2]\\n{q0,q1}\\n{q0,q2}\\n1.\\nDetermine transitions\\nδD\\n0\\n1\\n[q0]\\n[q0,q1]\\n[q0]\\n[q0,q1]\\n[q0,q1]\\n[q0,q2]\\n*[q0,q2]\\n[q0,q1]\\n[q0]\\n2.        Retain only those states \\nreachable from {q0}\\n0.\\nEnumerate all possible subsets'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 112, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='NFA to DFA: Repeating the example \\nusing LAZY CREATION\\n•\\nL = {w | w ends in 01}\\nq\\nq\\n0\\n0,1\\nq\\n1\\nNFA:\\nDFA:\\n[q0]\\n1\\n0\\n[q0,q1]\\n1\\n[q0,q2]\\n0\\n0\\n1\\n113\\nq0\\nq1\\nq2\\nδN\\n0\\n1\\nq0\\n{q0,q1}\\n{q0}\\nq1\\nØ\\n{q2}\\n*q2\\nØ\\nØ\\nδD\\n0\\n1\\n[q0]\\n[q0,q1]\\n[q0]\\n[q0,q1]\\n[q0,q1]\\n[q0,q2]\\n*[q0,q2]\\n[q0,q1]\\n[q0]\\nMain Idea:\\nIntroduce states as you go\\n(on a need basis)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 113, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Ex.  Convert the following Non-Deterministic Finite \\nAutomata (NFA) to Deterministic Finite Automata (DFA)-\\n114'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 114, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Ex Convert the following Non-Deterministic Finite Automata \\n(NFA) to Deterministic Finite Automata (DFA)-\\nδD\\n0\\n1\\n[q0]\\n[q0]\\n[q1, q2]\\n[q1, q2]\\n[q0,q1, q2]\\n[q1,q2]\\n*[q0, q1,q2]\\n[q0,q1, q2]\\n[q1,q2]\\n115'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 115, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Ex Convert the following Non-Deterministic Finite Automata \\n(NFA) to Deterministic Finite Automata (DFA)\\n116\\nδD\\n0\\n1\\n[q0]\\n[q1, q2]\\nΦ\\n*[q1, q2]\\n[q1, q2]\\n[q2]\\n[q2]\\n[q1, q2]\\n[q2]\\nΦ\\nΦ\\nΦ'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 116, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Construct Deterministic Finite Automata (DFA) from the given \\nTransition Table\\n117'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 117, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Construct Deterministic Finite Automata (DFA) from the given \\nTransition Table\\n118'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 118, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Correctness of subset construction\\nTheorem: If D is the DFA constructed from \\nNFA N by subset construction, then \\nL(D)=L(N)\\n• Proof:\\n– Show that δD\\n*({q0},w) ≡δN\\n*(q0,w} , for all w\\n119\\nD ( q0\\n)\\nN (q0\\n– Using induction on w’s length:\\n• Let w = xa\\n• δD\\n* ({q0},xa) ≡δD( δN\\n* (q0,x}, a ) ≡δN\\n* (q0,w}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 119, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='A few subtle properties of DFAs and \\nNFAs\\n• The machine never really terminates. \\n– It is always waiting for the next input symbol or making \\ntransitions.\\n• The machine decides when to consume the next symbol \\nfrom the input and when to ignore it\\nfrom the input and when to ignore it.\\n– (but the machine can never skip a symbol)\\n• => A transition can happen even without really consuming \\nan input symbol (think of consuming \\uf065as a free token) – if \\nthis happens, then it becomes an \\uf065-NFA (see next few \\nslides).\\n• A single transition cannot consume more than one (non-\\uf065) \\nsymbol.\\n120'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 120, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='FA with \\uf065-Transitions \\n• We can allow explicit \\uf065-transitions in finite \\nautomata\\n– i.e., a transition from one state to another state \\nwithout consuming any additional input symbol \\n– Explicit \\uf065-transitions between different states \\nintroduce non-determinism.\\n121\\n– Makes it easier sometimes to construct NFAs\\n• E.g. when constructing an NFA using RE\\nDefinition: \\uf065-NFAs are those NFAs with at \\nleast one explicit \\uf065-transition defined.\\n• \\uf065-NFAs have one more column in their transition \\ntable'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 121, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='NFA with epsilon transition\\nWe extend the class of NFAs by allowing instantaneous \\nε transitions −\\n• ε -NFAs add a convenient feature but (in a sense)\\nthey bring us nothing new. They do not extend the\\nclass of languages that can be represented.\\nh\\nd\\nl\\nh\\n• Both NFAs and ε -NFAs recognize exactly the same\\nlanguages.\\n122'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 122, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Eg.1\\nRE = 0*(01)*\\nL1 = 0*  \\nL2 =(01)*\\nExample of an \\uf065-NFA\\n0\\n0\\nE.g.2\\nRE = (aab)*(a+aba)*\\n9/18/2024\\n123\\np\\nq\\nr\\n1\\nε'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 123, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='\\uf065-NFA \\n• An \\uf065-NFA is also defined by the 5-tuple: \\n– {Q, ∑ , q0,F, δ }\\n• \\uf065-NFA consists of:\\n– Q ==> a finite set of states\\n– ∑==> a finite set of input symbols (alphabet)\\n124\\n∑ ==> a finite set of input symbols (alphabet)\\n– q0 ==> a start state\\n– F ==> set of accepting states \\n– δ ==> a transition function, which is a mapping between \\nQ x {∑ U {ε}} ==> subset of Q  (2Q)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 124, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Extension of δ for NFA\\n• Let M ={Q, ∑ , q0,F, δ } be an NFA\\n• The function δ* : Q x (∑*) ==> 2Q is defined as\\n• Basis: for any q ∈Q, δ*(q,\\uf065) = {q}\\nI d\\ni\\n125\\n• Induction:\\n– Let\\nδ*(q0,w) = {p1,p2…,pk}\\n– δ (pi,a) = Si for i=1,2...,k\\n– Then,   δ*(q0,wa) = S1 U S2 U … U Sk\\nor \\nδ*(q0,wa) = U δ(s,a)\\ns ∈ δ*(q0,w)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 125, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Extension of δ for \\uf065-NFA\\n• Let M ={Q, ∑ , q0,F, δ } be an NFA\\n• The function δ* : Q x (∑*U{\\uf065}) ==> 2Q is defined as\\n• Basis: for any q ∈Q, δ*(q,\\uf065) = \\uf065_closure{q}\\nI d\\ni\\n126\\n• Induction:\\n– Let\\nδ*(q0,w) = {p1,p2…,pk}\\n– δ (pi,a) = Si for i=1,2...,k\\n– Then,   δ*(q0,wa) = S1 U S2 U … U Sk\\nor \\nδ*(q0,wa) = \\uf065_closure( U δ(s,a))\\ns ∈ δ*(q0,w)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 126, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='NFA with epsilon transition\\nEpsilon (ε) - closure\\n• Epsilon closure for a given state X is a set of states \\nwhich can be reached from the states X with only (null) \\nor ε moves including the state X itself.\\n• In other words, ε -closure for a state can be obtained by \\nf h\\nl\\nf h\\nh h\\nunion operation of the ε -closure of the states which can \\nbe reached from X with a single ε move in a recursive \\nmanner.\\n127\\nFor the given example, ε closure are \\nas follow\\n•\\nε-closure(A)= {A, B,C}\\n•\\nε-closure(B)= {B,C}\\n•\\nε-closure(C)={C}'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 127, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Language of an \\uf065-NFA\\n• An \\uf065-NFA accepts w if there exists at \\nleast one path from the start state to an \\naccepting (or final) state that is labeled \\nby w\\n• L(N) = { w | δ*(q0 w) ∩F ≠Φ }\\n128\\nL(N)  { w | δ (q0,w) ∩F ≠ Φ }'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 128, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example of an \\uf065-NFA\\nL = {w | w is empty, or if non-empty will end in 01}\\n• \\uf065-closure of a state q, \\nECLOSE(q), is the set \\nof all states (including \\n0\\n0,1\\n1\\n129\\nδE\\n0\\n1\\n\\uf065\\n\\uf065_closer\\n*q’0\\nØ\\nØ\\n{q0}\\n{q’0,q0}\\nq0\\n{q0,q1}\\n{q0}\\nØ\\n{q0}\\nq1\\nØ\\n{q2}\\nØ\\n{q1}\\n*q2\\nØ\\nØ\\nØ\\n{q2}\\nECLOSE(q’0)\\nECLOSE(q0)\\nitself) that can be \\nreached from q by \\nrepeatedly making an \\narbitrary number of \\uf065-\\ntransitions.  \\nstart\\nq0\\nq1\\nq2\\nq’0\\n\\uf065\\nECLOSE(q1)\\nECLOSE(q2)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 129, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example of an \\uf065-NFA\\nL = {w | w is empty, or if non-empty will end in 01}\\nSimulate for w=101:\\n0\\n0,1\\n1\\nTo simulate any transition:\\nStep 1) Go to all immediate destination states.\\nStep 2) From there go to all their \\uf065-closure states as well.\\n130\\nδE\\n0\\n1\\n\\uf065\\n*q’0\\nØ\\nØ\\n{q’0,q0}\\nq0\\n{q0,q1}\\n{q0}\\n{q0}\\nq1\\nØ\\n{q2}\\n{q1}\\n*q2\\nØ\\nØ\\n{q2}\\nECLOSE(q’0)\\nECLOSE(q0)\\nstart\\nq0\\nq1\\nq2\\nq’0\\n\\uf065\\nq0’\\nq0\\nq0’\\n\\uf065\\n\\uf065\\nq1\\n0\\nq2\\n1\\nq0\\n1\\nØ\\n1\\nx'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 130, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example of another \\uf065-NFA\\nSimulate for w=101:\\n?\\nq0\\nq1\\n0\\n0,1\\n1\\nq2\\n\\uf065\\n\\uf065\\n1\\nTo simulate any transition:\\nStep 1) Go to all immediate destination states.\\nStep 2) From there go to all their \\uf065-closure states as well.\\n131\\nδE\\n0\\n1\\n\\uf065\\n\\uf065_closer\\n*q’0\\nØ\\nØ\\n{q0}\\n{q’0,q0,q3}\\nq0\\n{q0,q1}\\n{q0}\\n{q3}\\n{q0,q3}\\nq1\\nØ\\n{q2}\\nØ\\n{q1}\\n*q2\\nØ\\nØ\\nØ\\n{q2}\\nq3\\nØ\\n{q2}\\nØ\\n{q3}\\nstart\\nq’0\\nq3'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 131, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n• Calculate ε –closure(s) \\n= {s, w, q0,p, t}\\n132'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 132, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Calculate δ*(q0,w) for w=aba \\n9/18/2024\\n133'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 133, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Equivalency of DFA, NFA, \\uf065-NFA\\n• Theorem: A language L is accepted by some \\n\\uf065-NFA if and only if L is accepted by some DFA\\n• Theorem: A language L is accepted by some \\uf065-\\nNFA then there is an NFA that also accepts L.\\n134\\n• Implication:\\n– DFA ≡NFA ≡\\uf065-NFA\\n– (all accept Regular Languages)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 134, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Eliminating \\uf065-transitions\\nTheorem: For every language L⊆\\uf053* accepted by an \\uf065-NFA \\nE = {Q,∑,δE,q0,FE}, there is an NFA N with no \\uf065-\\ntransitions that also accepts L.\\nLet E = {Q,∑,δE,q0,FE} be an \\uf065-NFA\\nGoal: To build NFA N={Q,∑,δN,q0,FN} s.t. L(N)=L(E)\\n135\\nConstruction:\\nfor every q\\uf0ceQ, δN (q, \\uf065)= Φ and for every q\\uf0ceQ and \\nevery a\\uf0ce∑,\\nδN (q, a)= δE *(q, a)\\nAnd we define\\nFN = FE U {q0}   if \\uf065\\uf0ceL.\\n= FE\\nif not.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 135, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content=\"Convert the given NFA with epsilon to NFA without \\nepsilon.\\nStep:1\\nε-closure(q0) = {q0,q1,q2}\\nε-closure(q1) = {q1,q2}\\nε-closure(q2) = {q2}\\nStep:2 Now we will obtain ’ transitions for each state on each input symbol as shown \\nbelow\\nδ'(q0, 2) = ε-closure(δ(δ*(q0, ε),2\\n= ε-closure(δ(q0,q1,q2), 2\\n= ε-closure(δ(q0, 2) ∪δ(q\\n= ε-closure(Φ U ΦU q2)\\n= ε-closure(q2)\\n= {q2}\\n136\\nbelow\\nδ'(q0, 0) = ε-closure(δ(δ*(q0, ε),0))\\n= ε-closure(δ(ε-closure(q0),0))\\n= ε-closure(δ(q0,q1,q2), 0))\\n= ε-closure(δ(q0, 0) \\nδ(q1, 0) \\nδ(q2, 0) )\\n= ε-closure(q0 \\nΦ \\nΦ)\\n= ε-closure(q0)\\n= {q0,q1, q2}\\nδ'(q0, 1) = ε-closure(δ(δ*(q0, ε),1))\\n= ε-closure(δ(q0,q1,q2), 1))\\n= ε-closure(δ(q0, 1) \\nδ(q1, 1) \\nδ(q2, 1) )\\n= ε-closure(Φ q1 \\nΦ)\\n= ε-closure(q1)\\n= {q1, q2}\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 136, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content=\"Convert the given NFA with epsilon to NFA without epsilon.\\nδ'(q1, 0) = ε-closure(δ(δ*(q1, ε),0))\\n= ε-closure(δ(q1,q2), 0))\\n= ε-closure(δ(q1, 0) U δ(q2, 0) )\\n= ε-closure(Φ ∪Φ)\\n= ε-closure(Φ)\\n= Φ\\nStep:1\\nε-closure(q0) = {q0,q1,q2}\\nε-closure(q1) = {q1,q2}\\nε-closure(q2) = {q2}\\nδ'(q2, 0) = ε-closure(δ(δ*(q2, ε),0))\\n= ε-closure(δ(q2), 0))\\n= ε-closure(δ(q2, 0))\\n= ε-closure(Φ)\\n= Φ\\nStep:3 Now, we will \\nsummarize all the computed \\nδ' transitions as given below −\\nδ'(q0,0)={q0,q1,q2}\\nδ'(q0,1)={q1,q2}\\nδ'( 0 2) { 2}\\n137\\n Φ\\nδ'(q1,1) = ε-closure(δ(δ*(q1, ε),1))\\n= ε-closure(δ(q1,q2), 1))\\n= ε-closure(δ(q1, 1) U δ(q2, 1) )\\n= ε-closure(q1 ∪Φ)\\n= ε-closure(q1)\\n= {q1,q2}\\nδ'(q1, 2) = ε-closure(δ(δ*(q1, ε),2))\\n= ε-closure(δ(q1,q2), 2))\\n= ε-closure(δ(q1, 2) U δ(q2, 2) )\\n= ε-closure(Φ ∪q2)\\n= ε-closure(q2)\\nδ'(q2, 1) = ε-closure(δ(δ*(q2, ε),1))\\n= ε-closure(δ(q2), 1)\\n= ε-closure(δ(q2, 1))\\n= ε-closure(Φ)\\n= Φ\\nδ'(q2, 2) = ε-closure(δ(δ*(q2, ε),))\\n= ε-closure(δ(q2), 2))\\n= ε-closure(δ(q2, 2))\\n= ε-closure(q2)\\n= {q2}\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 136, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content=\"= ε-closure(δ(q2), 1)\\n= ε-closure(δ(q2, 1))\\n= ε-closure(Φ)\\n= Φ\\nδ'(q2, 2) = ε-closure(δ(δ*(q2, ε),))\\n= ε-closure(δ(q2), 2))\\n= ε-closure(δ(q2, 2))\\n= ε-closure(q2)\\n= {q2}\\nδ'(q0,2)={q2}\\nδ'(q1,0)= { Φ }\\nδ'(q1,1)={q1,q2}\\nδ'(q1,2)={q2}\\nδ'(q2,0)={ Φ }\\nδ'(q2,1)={ Φ }\\nδ'(q2,2)={q2}\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 137, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content=\"Convert the given NFA with epsilon to NFA without epsilon.\\nStep:1\\nε-closure(q0) = {q0,q1,q2}\\nε-closure(q1) = {q1,q2}\\nε-closure(q2) = {q2}\\nStep:3 Now, we will \\nsummarize all the computed \\nδ' transitions as given below −\\nδ'(q0,0)={q0,q1,q2}\\nδ'( 0 1) { 1 2}\\nStep:4-The transition table for NFA without \\nepsilon is given below-\\n138\\nδ'(q0,1)={q1,q2}\\nδ'(q0,2)={q2}\\nδ'(q1,0)= { Φ }\\nδ'(q1,1)={q1,q2}\\nδ'(q1,2)={q2}\\nδ'(q2,0)={ Φ }\\nδ'(q2,1)={ Φ }\\nδ'(q2,2)={q2}\\nStates\\\\input\\ns\\n0\\n1\\n2\\nq0\\n{q0,q1,q2}\\n{q1,q2}\\n{q2}\\nq1\\nΦ\\n{q1,q2}\\n{q2}\\nq2\\nΦ\\nΦ\\n{q2}\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 138, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Convert the given NFA with epsilon to DFA.\\n139'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 139, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Eliminating \\uf065-transitions\\nLet E = {QE,∑,δE,q0,FE} be an \\uf065-NFA\\nGoal: To build DFA D={QD,∑,δD,{qD},FD} s.t. L(D)=L(E)\\nConstruction:\\n1.\\nQD= all reachable subsets of QE factoring in \\uf065-\\nl\\n140\\nclosures\\n2.\\nqD = ECLOSE(q0)\\n3.\\nFD=subsets S in QD s.t. S∩FE≠Φ\\n4.\\nδD: for each subset S of QE and for each input \\nsymbol a\\uf0ce∑: \\n•\\nLet r= U δE(p,a)\\n// go to destination states\\n•\\nδD(S,a) = U ECLOSE(r)// from there, take a union\\nof all their \\uf065-closures\\np in s\\nr in R'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 140, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='ε-NFA to DFA\\n• Steps for converting NFA with ε to DFA:\\n• Step 1: We will take the ε-closure for the starting state \\nof NFA as a starting state of DFA.\\n• Step 2: Find the states for each input symbol that can be \\ntraversed from the present. That means the union of \\ntransition value and their closures for each state of NFA \\npresent in the current state of DFA\\npresent in the current state of DFA.\\n• Step 3: If we found a new state, take it as current state \\nand repeat step 2.\\n• Step 4: Repeat Step 2 and Step 3 until there is no new \\nstate present in the transition table of DFA.\\n• Step 5: Mark the states of DFA as a final state which \\ncontains the final state of NFA.\\n9/18/2024\\n141'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 141, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content=\"Convert the given NFA with epsilon to DFA \\nε-closure {q0} = {q0, q1, q2}  \\nε-closure {q1} = {q1}  \\nε-closure {q2} = {q2}  \\nε-closure {q3} = {q3}  \\nε-closure {q4} = {q4} \\nStep:1 find ε-closure of initial state\\nε-closure {q0} = {q0, q1, q2}\\nlet say it as A\\nδ'(B, 0) = ε-closure {δ(q3, 0) }\\n= ϕ\\nδ'(B, 1) = ε-closure {δ(q3, 1) }\\n142\\n{q }\\n{q , q , q }\\ny\\nStep: 2\\nδ'(A, 0)  = ε-closure {δ((q0, q1, q2), 0) } \\n= ε-closure {δ(q0, 0) ∪δ(q1, 0) ∪δ(q2, 0)} \\n= ε-closure {q3}\\n= {q3} call it as state B.\\nδ'(A, 1)   = ε-closure {δ((q0, q1, q2), 1) } \\n= ε-closure {δ((q0, 1) ∪δ(q1, 1) ∪δ(q2, 1)}\\n= ε-closure {q3} \\n= {q3} = B. \\nδ (B, 1)\\nε closure {δ(q3, 1) }\\n= ε-closure {q4}\\n= {q4}\\ni.e. state C\\nδ'(C, 0) = ε-closure {δ(q4, 0) }\\n= ϕ\\nδ'(C, 1) = ε-closure {δ(q4, 1) }\\n= ϕ\"),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 142, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example:Convert ε-NFA to DFA\\n9/18/2024\\n143'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 143, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example: \\uf065-NFA \\uf0e8DFA\\nε-NFA\\nNFA\\n144\\nq\\nδ(q,a)\\nδ(q,b)\\nδ(q, ε)\\n1\\nϕ\\nϕ\\n{2}\\n2\\n{2,3}\\nϕ\\nϕ\\n3\\nϕ\\n{4}\\nϕ\\n4\\nϕ\\n{5}\\n{1}\\n5\\n{4}\\nϕ\\nϕ\\nδ*(q,a)\\nδ*(q,b)\\n{2,3}\\nϕ\\n{2,3}\\nϕ\\nϕ\\n{1,2,4}\\n{2,3}\\n{5}\\n{1,2,4}\\nϕ'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 144, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='δ*(q,a)\\nδ*(q,b)\\n{2,3}\\nϕ\\n{2,3}\\nϕ\\n{\\n}\\nδ(q,a)\\nδ(q,b)\\n{2,3}\\nϕ\\n{2,3}\\n{1,2,4}\\nExample: NFA \\uf0e8DFA\\nq\\n{1}\\n{2,3}\\nq\\n1\\n2\\n3\\n145\\nϕ\\n{1,2,4}\\n{2,3}\\n{5}\\n{1,2,4}\\nϕ\\nϕ\\nϕ\\n{2,3}\\n{5}\\n{1,2,4}\\nϕ\\nϕ\\n{1,2,4}\\n{5}\\n3\\n4\\n5'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 145, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example: \\uf065-NFA \\uf0e8NFA \\uf0e8DFA\\n146'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 146, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='DFA\\n9/18/2024\\n147'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 147, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Minimization of DFA\\n9/18/2024\\n148'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 148, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Minimization of DFA using Equivalence \\nTheorem\\n• Step 1: We will divide Q (set of states) into two sets. \\nOne set will contain all final states and other set will \\ncontain non-final states. This partition is called P0.\\n• Step 2: Initialize k = 1\\n• Step 3: Find Pk by partitioning the different sets of Pk-1. \\nIn each set of P\\nwe will take all possible pair of states\\nIn each set of Pk-1, we will take all possible pair of states. \\nIf two states of a set are distinguishable, we will split the \\nsets into different sets in Pk.\\n• Step 4: Stop when Pk = Pk-1 (No change in partition)\\n• Step 5: All states of one set are merged into one. No. of \\nstates in minimized DFA will be equal to no. of sets in Pk.\\n9/18/2024\\n149\\nTwo states ( qi, qj ) are distinguishable in partition Pk if for any input symbol a, δ \\n( qi, a ) and δ ( qj, a ) are in different sets in partition Pk-1.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 149, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Minimization of DFA\\na\\nb\\n->q0\\nq1\\nq2\\nq1\\nq1\\nq3\\nq2\\nq1\\nq2\\nq3\\nq1\\nq4\\n*q4\\nq1\\nq2\\n9/18/2024\\n150\\nP0 = { q0 , q1 , q2 , q3 } { q4 }\\nP1 = { q0 , q1 , q2 } { q3 } { q4 }\\nP2 = { q0 , q2 } { q1 } { q3 } { q4 }\\nP3 = { q0 , q2 } { q1 } { q3 } { q4 }\\nSince P3 = P2, so we stop.\\nFrom P3, we infer that states q0 and q2 are equivalent and \\ncan be merged together.'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 150, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Minimization of DFA\\na\\nb\\n->q0\\nq1\\nq2\\nq1\\nq1\\nq3\\nq2\\nq1\\nq2\\nq3\\nq1\\nq4\\n*q4\\nq1\\nq2\\n9/18/2024\\n151'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 151, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Minimization of DFA – Table Filling Method\\n• Draw a table for all pairs of states (P,Q)\\n• Mark all pairs where P\\uf0ceF and Q\\uf0cfF.\\n• If there are any unmarked pairs (P,Q) such \\nthat [δ(P,x), δ(Q,x)] is marked, then mark \\n[P Q] where ‘x’ is an input symbol Repeat this\\n[P,Q], where x  is an input symbol. Repeat this \\nuntil no more markings can be made.\\n• Combine all the unmarked pairs and make them \\na single state in the minimized DFA.\\n152'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 152, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n9/18/2024\\n153\\nStep 1: Draw a table for all pairs of states (P, Q)'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 153, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='• Step 2: Mark all pairs where P\\uf0ceF and Q\\uf0cfF.\\n• Step 3: If there are any Unmarked pairs (P, Q) \\nsuch that [δ(P, x),δ(Q, x)] is marked, then mark \\n[P, Q] where ‘x’ is an input symbol. Repeat this \\nuntil no more marking can be made.\\n9/18/2024\\n154'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 154, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='•\\n9/18/2024\\n155'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 155, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='• Step 4 :(A, C), B, D, E\\n9/18/2024\\n156'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 156, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n157'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 157, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n158'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 158, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Example\\n159'),\n",
       " Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': '', 'creationdate': '2024-09-18T14:45:11+05:30', 'source': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'file_path': '..\\\\data\\\\pdf\\\\1_Final_DFA.pdf', 'total_pages': 160, 'format': 'PDF 1.7', 'title': 'Microsoft PowerPoint - 1_Final_DFA (2) (4) [Compatibility Mode]', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-09-18T14:45:11+05:30', 'trapped': '', 'modDate': \"D:20240918144511+05'30'\", 'creationDate': \"D:20240918144511+05'30'\", 'page': 159, 'source_file': '1_Final_DFA.pdf', 'file_type': 'pdf'}, page_content='Summary\\n•\\nDFA\\n– Definition\\n– Transition diagrams & tables\\n•\\nRegular language\\n•\\nNFA\\n– Definition\\n– Transition diagrams & tables\\n•\\nDFA vs. NFA\\n160\\nDFA vs. NFA\\n•\\nNFA to DFA conversion using subset construction\\n•\\nEquivalency of DFA & NFA\\n•\\nRemoval of redundant states and including dead states\\n• \\uf065-transitions in NFA\\n•\\nDFA Minimization'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\\nfor the Transformer architecture and the modern era of large language models.\\n2. The Core Intuition\\nThink of attention like a search engine. You have a query (what you're looking for), a set of keys (labels\\nor identifiers for stored information), and values (the actual information stored). The attention\\nmechanism computes a similarity score between the query and each key, converts these scores into\\nprobabilities using softmax, and returns a weighted sum of the values.\\nThe general attention formula is:\\nAttention(Q, K, V) = softmax(QKT / sqrt(d_k)) * V\\nWhere Q is the query matrix, K is the key matrix, V is the value matrix, and d_k is the dimension of the\\nkeys. The division by sqrt(d_k) prevents the dot products from growing too large in magnitude, which\\nwould push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='would push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\\nSoft attention computes a weighted average over all input positions — it is fully differentiable and can\\nbe trained end-to-end with backpropagation. Hard attention, on the other hand, selects a single input\\nposition at each step stochastically. While hard attention can be more efficient, it requires reinforcement\\nlearning techniques to train since it is not differentiable.\\n3.2 Self-Attention (Intra-Attention)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='Self-attention allows a sequence to attend to itself. Every token in the input sequence computes\\nattention scores with every other token, capturing relationships within the same sequence. This is\\nextremely powerful for tasks like understanding pronoun reference, long-range dependencies, and\\nsyntactic structure. Self-attention is the cornerstone of the Transformer architecture.\\n3.3 Cross-Attention\\nCross-attention is used when queries come from one sequence (e.g., the decoder) and keys/values\\ncome from another sequence (e.g., the encoder output). This is the classic encoder-decoder attention\\nused in machine translation and image captioning tasks.\\n3.4 Multi-Head Attention\\nInstead of performing a single attention function, multi-head attention runs h parallel attention\\noperations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='operations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation\\nsubspaces at different positions.\\nMultiHead(Q,K,V) = Concat(head_1, ..., head_h) * W_O\\nwhere head_i = Attention(Q*W_Q_i, K*W_K_i, V*W_V_i)\\n4. Why Attention Changed Everything\\nBefore attention, sequence-to-sequence models used RNNs and LSTMs which suffered from the\\nvanishing gradient problem and struggled to capture long-range dependencies. Attention solved this by\\nproviding direct connections between any two positions in the sequence, regardless of distance. This\\nmade training faster, more parallelizable (unlike RNNs which are sequential), and more interpretable\\nsince we can visualize which tokens the model attends to.\\n5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field\\nAny token can directly attend to any other token\\nInterpretability\\nAttention weights provide insight into model decisions\\nPermutation Equivariant\\nNo built-in notion of order (positional encoding needed)\\nQuadratic Complexity\\nO(n^2) cost w.r.t. sequence length — a known limitation\\n6. Summary\\nAttention mechanisms revolutionized deep learning by enabling models to dynamically focus on\\nrelevant information. From the original Bahdanau attention to modern multi-head self-attention, these\\ntechniques power state-of-the-art NLP, vision, and multimodal systems. Understanding attention is'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='essential for anyone working with Transformers, BERT, GPT, or any modern AI system.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"Transformer Architecture & Self-Attention\\nDeep Dive into 'Attention is All You Need'\\n1. The Transformer Model\\nThe Transformer, introduced by Vaswani et al. in 2017 in the landmark paper 'Attention is All You\\nNeed', completely replaced recurrence and convolutions with attention mechanisms. It consists of an\\nencoder stack and a decoder stack, each made up of identical layers. This architecture became the\\nfoundation for BERT, GPT, T5, and virtually every state-of-the-art NLP model today.\\n2. Encoder Architecture\\nThe encoder is composed of N identical layers (typically N=6). Each layer has two sub-layers: a\\nmulti-head self-attention mechanism, and a position-wise fully connected feed-forward network. Each\\nsub-layer has a residual connection followed by layer normalization.\\nLayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"LayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\\npassing through the attention or FFN transformations, making it much easier to train deep networks.\\n3. Positional Encoding\\nSince self-attention has no inherent notion of order (it's permutation equivariant), positional encodings\\nare added to the input embeddings to inject information about token positions. The original Transformer\\nused sinusoidal positional encodings:\\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\nSinusoidal encodings have the nice property that the encoding for position pos+k can be expressed as\\na linear function of the encoding at pos, which may help the model generalize to unseen sequence\\nlengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"lengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\\nHere's exactly how scaled dot-product attention works:\\nStep\\nOperation\\nShape\\n1\\nProject input X into Q, K, V matrices\\n(seq_len, d_model) → (seq_len, d_k)\\n2\\nCompute dot product: Q × K^T\\n(seq_len, seq_len)\\n3\\nScale by 1/sqrt(d_k)\\n(seq_len, seq_len)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"4\\nApply optional mask (for decoder)\\n(seq_len, seq_len)\\n5\\nApply softmax row-wise\\n(seq_len, seq_len) — rows sum to 1\\n6\\nMultiply by V\\n(seq_len, d_v)\\n5. The Decoder and Masked Attention\\nThe decoder has three sub-layers per block: a masked multi-head self-attention layer, a cross-attention\\nlayer (attending to the encoder output), and a feed-forward network. The masking in the first sub-layer\\nprevents the decoder from attending to future positions — this is critical for autoregressive generation\\nwhere the model must predict one token at a time without 'cheating' by looking at future tokens.\\n6. Feed-Forward Networks in Transformers\\nEach Transformer layer contains a position-wise FFN applied independently to each position:\\nFFN(x) = max(0, x*W_1 + b_1) * W_2 + b_2\\nTypically the inner dimension d_ff = 4 * d_model (e.g., 2048 when d_model=512). This FFN is where\\nmuch of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"much of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\\n7. Transformer Hyperparameters (Original Paper)\\nHyperparameter\\nBase Model\\nLarge Model\\nd_model (embedding dim)\\n512\\n1024\\nNumber of layers (N)\\n6\\n6\\nNumber of heads (h)\\n8\\n16\\nd_k = d_v\\n64\\n64\\nd_ff (FFN inner dim)\\n2048\\n4096\\nDropout rate\\n0.1\\n0.3\\nParameters\\n65M\\n213M\\n8. Why Transformers Beat RNNs\\nRNNs process sequences token by token, making parallelization impossible during training. LSTMs\\nmitigate vanishing gradients but still struggle with very long sequences. Transformers process all\\ntokens simultaneously, support full parallelism on GPUs/TPUs, handle long-range dependencies with\\nO(1) path length between any two positions, and scale remarkably well with data and compute —\\nleading to the era of large language models.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='the attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='idea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)\\nThis allows the attention computation to be rewritten using matrix associativity, computing KV\\naggregations first and then applying Q — avoiding the n×n attention matrix entirely. Examples include\\nPerformer (using random Fourier features) and Linear Transformer.\\n4. Flash Attention'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Flash Attention (Dao et al., 2022) takes a different approach — instead of approximating attention, it\\ncomputes exact attention but uses a hardware-aware algorithm to minimize memory I/O. The key\\ninsight is that the bottleneck in standard attention is not compute but memory bandwidth between HBM\\n(high-bandwidth memory) and SRAM (fast on-chip memory).\\nFlash Attention uses tiling to split Q, K, V into blocks and processes them in SRAM, never materializing\\nthe full n×n attention matrix in HBM. This achieves 2-4x speedup and O(n) memory usage while\\ncomputing mathematically identical results to standard attention. Flash Attention 2 and 3 further\\nimproved performance with better parallelism strategies.\\n5. Multi-Query and Grouped-Query Attention\\n5.1 Multi-Query Attention (MQA)\\nMulti-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Multi-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —\\ncritical for autoregressive generation where cached keys and values consume large amounts of\\nmemory. MQA is used in models like PaLM and Falcon.\\n5.2 Grouped-Query Attention (GQA)\\nGrouped-Query Attention (Ainslie et al., 2023) is a middle ground between MHA and MQA. Query\\nheads are divided into G groups, and each group shares one key and value head. GQA achieves\\nquality close to MHA with inference speed close to MQA. It is used in Llama 2, Llama 3, Mistral, and\\nmany modern open-source LLMs.\\n6. Relative Positional Encodings\\n6.1 Rotary Position Embedding (RoPE)\\nRoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='RoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance\\n(m-n), giving the model a natural sense of relative position. RoPE generalizes well to longer sequences\\nthan seen during training and is used in GPT-NeoX, LLaMA, Falcon, and most modern LLMs.\\n6.2 ALiBi (Attention with Linear Biases)\\nALiBi (Press et al., 2021) adds a linear bias to attention scores based on the distance between query\\nand key positions: score(q_i, k_j) -= m * |i - j|, where m is a head-specific slope. This penalizes\\nattending to distant tokens, encouraging locality. ALiBi models extrapolate well to longer sequences\\nwithout any retraining.\\n7. Comparison of Attention Variants'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Method\\nComplexity\\nExact?\\nUse Case\\nStandard MHA\\nO(n^2)\\nYes\\nGeneral purpose, short sequences\\nLongformer\\nO(n * w)\\nApprox\\nLong documents\\nBigBird\\nO(n)\\nApprox\\nVery long sequences\\nFlash Attention\\nO(n^2) compute, O(n) mem\\nYes\\nGPU-efficient training\\nLinear Attention\\nO(n)\\nApprox\\nExtreme length, efficiency\\nMQA / GQA\\nO(n^2)\\nYes\\nFast inference, small KV cache\\n8. Summary & Takeaways\\nThe field of attention mechanisms has evolved rapidly since 2017. Standard multi-head attention\\nremains the gold standard for quality, while efficient variants like Flash Attention, GQA, and sparse\\nattention methods make it practical to scale to longer sequences and larger models. Choosing the right\\nattention variant depends on your sequence length, hardware constraints, inference requirements, and\\nacceptable quality trade-offs. Modern LLMs like Llama 3 and Mistral combine Flash Attention + GQA +\\nRoPE for an optimal balance of quality, speed, and memory efficiency.')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aa574",
   "metadata": {},
   "source": [
    "## convert the text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3dadacc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings fro 180 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d91f83e8f8b480b983fa9577e17c199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(180, 384)\n",
      "adding 180 document to vector store...\n",
      "error adding documents to vector store: Non-empty lists are required for ['ids'] in add.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Non-empty lists are required for ['ids'] in add.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m embeddings=embedding_manager.generate_embeddings(texts)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# store in the vecor db\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, embeddings)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33merror adding documents to vector store: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, embeddings)\u001b[39m\n\u001b[32m     72\u001b[39m     embeddings_list.append(embedding.tolist()) \u001b[38;5;66;03m# convert the embedding from a numpy array to a list so that it can be stored in ChromaDB, which expects embeddings to be in list format.\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments_text\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msuccesfully added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents to vector store\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtotal documnets in collection:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.collection.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RAG_pipeline\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:97\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd\u001b[39m(\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     68\u001b[39m     ids: OneOrMany[ID],\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     uris: Optional[OneOrMany[URI]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     79\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Add records to the collection.\u001b[39;00m\n\u001b[32m     81\u001b[39m \n\u001b[32m     82\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03m        ValueError: If an ID already exists.\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     add_request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_prepare_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m._client._add(\n\u001b[32m    107\u001b[39m         collection_id=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    108\u001b[39m         ids=add_request[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m         database=\u001b[38;5;28mself\u001b[39m.database,\n\u001b[32m    115\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RAG_pipeline\\venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:103\u001b[39m, in \u001b[36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> T:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    105\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RAG_pipeline\\venv\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:235\u001b[39m, in \u001b[36mCollectionCommon._validate_and_prepare_add_request\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m    225\u001b[39m add_records = normalize_insert_record_set(\n\u001b[32m    226\u001b[39m     ids=ids,\n\u001b[32m    227\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m     uris=uris,\n\u001b[32m    232\u001b[39m )\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[43mvalidate_insert_record_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m validate_record_set_contains_any(record_set=add_records, contains_any={\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Prepare\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RAG_pipeline\\venv\\Lib\\site-packages\\chromadb\\api\\types.py:442\u001b[39m, in \u001b[36mvalidate_insert_record_set\u001b[39m\u001b[34m(record_set)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_insert_record_set\u001b[39m(record_set: InsertRecordSet) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    439\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    440\u001b[39m \u001b[33;03m    Validates the InsertRecordSet, ensuring that all fields are of the right type and length.\u001b[39;00m\n\u001b[32m    441\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[43m_validate_record_set_length_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     validate_base_record_set(record_set)\n\u001b[32m    445\u001b[39m     validate_ids(record_set[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\RAG_pipeline\\venv\\Lib\\site-packages\\chromadb\\api\\types.py:465\u001b[39m, in \u001b[36m_validate_record_set_length_consistency\u001b[39m\u001b[34m(record_set)\u001b[39m\n\u001b[32m    458\u001b[39m zero_lengths = [\n\u001b[32m    459\u001b[39m     key\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, lst \u001b[38;5;129;01min\u001b[39;00m record_set.items()\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lst) == \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    462\u001b[39m ]\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m zero_lengths:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNon-empty lists are required for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzero_lengths\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m    468\u001b[39m     error_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    469\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lst)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, lst \u001b[38;5;129;01min\u001b[39;00m record_set.items()\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m lst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    472\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Non-empty lists are required for ['ids'] in add."
     ]
    }
   ],
   "source": [
    "# extraxt the text\n",
    "texts=[doc.page_content for doc in chunks] # extract the text content from each document chunk to create a list of strings that can be passed to the embedding model for generating embeddings. Each element in the \"texts\" list corresponds to the text content of a document chunk, which will be embedded and stored in the vector store for later retrieval during RAG operations.\n",
    "\n",
    "# generate the embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# store in the vecor db\n",
    "vector_store.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0eee3",
   "metadata": {},
   "source": [
    "### Retrival Pipeline From Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771bdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetrival at 0x2340f67bb60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetrival:\n",
    "    \n",
    "    def __init__(self,vector_store:VectorStore,embedding_manager:EmbeddingManager):\n",
    "        \"\"\"Initailize the retriver\n",
    "        Args:\n",
    "        vector_store: instance of the VectorStore class that manages the vector database for storing and retrieving document embeddings.\n",
    "        embedding_manager: instance of the EmbeddingManager class that handles generating embeddings for query texts.\n",
    "        \"\"\"\n",
    "        self.vector_store=vector_store\n",
    "        self.embedding_manager=embedding_manager \n",
    "\n",
    "    def retrive(self,query:str,top_k:int=5,score_threshold:float=0.0)-> List[Dict[str,Any]]:\n",
    "        \"\"\"\n",
    "        retrive relevent dpcument for a  query\n",
    "        \n",
    "        Args:\n",
    "        query; the input query comes form the user that we want to find relevant documents for. This is typically a natural language question or statement that the user inputs to the RAG system.\n",
    "        top_k; top k results to return. This parameter controls how many of the most relevant documents will be returned by the retriever. A higher value of top_k will return more documents, but may also include less relevant ones, while a lower value will return fewer but more relevant documents.\n",
    "        score_threashold: the minimum cosine similarity score for a document to be considered relevant and included in the results. This parameter helps filter out documents that are not sufficiently similar to the query, ensuring that only documents with a cosine similarity score above this threshold are returned in the results.\n",
    "\n",
    "        Returns:\n",
    "        A list of dictionaries, where each dictionary contains the retrieved document's text, metadata, and its cosine similarity score with respect to the query. The list is sorted in descending order of relevance (highest cosine similarity score first).\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"retriving document for the query {query} with top_k={top_k} and score_threshold={score_threshold}\")\n",
    "\n",
    "        #generate embedding for the query\n",
    "        query_embedding=self.embedding_manager.generate_embeddings([query])[0] # generate embedding for the query and take the first element of the resulting array since we are only embedding one query at a time.\n",
    "\n",
    "        # search in the vector store\n",
    "        try:\n",
    "            results=self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()], # convert the query embedding from a numpy array to a list so that it can be used in the ChromaDB query, which expects embeddings to be in list format.\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            #process results\n",
    "            retrived_docs=[]\n",
    "\n",
    "            if results['documents'] and results['documents'][0]: # check if there are any documents in the results and if the first element of the documents list is not empty\n",
    "                documents=results['documents'][0]\n",
    "                metadatas=results['metadatas'][0]\n",
    "                distances=results['distances'][0] # retrieve the cosine similarity scores (distances) for the retrieved documents from the query results. This can be useful for filtering the results based on relevance and for debugging purposes.\n",
    "                ids=results['ids'][0] # retrieve the list of document IDs from the query results. This can be useful for debugging and for retrieving the original documents later if needed.\n",
    "\n",
    "                for i,(doc_id,document,metadata,distance) in enumerate(zip(ids,documents,metadatas,distances)):  \n",
    "                    # convert distcnce t0o similarity score\n",
    "                    similarity_score=1-distance # since cosine similarity is 1 - cosine distance, we can convert the distance to a similarity score by subtracting the distance from 1. This gives us a similarity score that ranges from 0 to 1, where 1 means the document is identical to the query and 0 means it is completely different.\n",
    "\n",
    "                    if similarity_score>=score_threshold:\n",
    "                        retrived_docs.append(\n",
    "                            {\n",
    "                                'id':doc_id,\n",
    "                                'content':document,\n",
    "                                'metadata':metadata,\n",
    "                                'similarity_score':similarity_score,\n",
    "                                'distance':distance,\n",
    "                                'rank':i+1\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                print(f\"retrived {len(retrived_docs)} documents after filtering\")\n",
    "            else:\n",
    "                print(\"no document found\")\n",
    "\n",
    "            return retrived_docs\n",
    "        except Exception as e:\n",
    "            print(f\"error while retrieving documents: {e}\")\n",
    "            return []\n",
    "               \n",
    "\n",
    "\n",
    "rag_retriver=RAGRetrival(vector_store,embedding_manager)\n",
    "rag_retriver\n",
    " \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving document for the query Sparse Attention with top_k=5 and score_threshold=0.0\n",
      "Generating embeddings fro 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3144c1a782a4f07bbaf4a3b21e7523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(1, 384)\n",
      "retrived 5 documents after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_1e783488_12',\n",
       "  'content': 'Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token',\n",
       "  'metadata': {'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'author': '(anonymous)',\n",
       "   'page': 0,\n",
       "   'title': '(anonymous)',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'format': 'PDF 1.4',\n",
       "   'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'creator': '(unspecified)',\n",
       "   'context_length': 964,\n",
       "   'doc_index': 12,\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'total_pages': 3,\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'source_file': '03_Advanced_Attention_Variants.pdf',\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'subject': '(unspecified)'},\n",
       "  'similarity_score': 0.2736407518386841,\n",
       "  'distance': 0.7263592481613159,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_c9a3a63f_12',\n",
       "  'content': 'Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token',\n",
       "  'metadata': {'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'source_file': '03_Advanced_Attention_Variants.pdf',\n",
       "   'author': '(anonymous)',\n",
       "   'title': '(anonymous)',\n",
       "   'doc_index': 12,\n",
       "   'trapped': '',\n",
       "   'page': 0,\n",
       "   'creator': '(unspecified)',\n",
       "   'subject': '(unspecified)',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'keywords': '',\n",
       "   'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'context_length': 964,\n",
       "   'total_pages': 3},\n",
       "  'similarity_score': 0.2736407518386841,\n",
       "  'distance': 0.7263592481613159,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_68259f1e_0',\n",
       "  'content': \"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\",\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'author': '(anonymous)',\n",
       "   'source_file': '01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'format': 'PDF 1.4',\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'doc_index': 0,\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'title': '(anonymous)',\n",
       "   'context_length': 924,\n",
       "   'creator': '(unspecified)',\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'total_pages': 3,\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'page': 0,\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'subject': '(unspecified)',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.19435656070709229,\n",
       "  'distance': 0.8056434392929077,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3faf6367_0',\n",
       "  'content': \"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\",\n",
       "  'metadata': {'creator': '(unspecified)',\n",
       "   'doc_index': 0,\n",
       "   'total_pages': 3,\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '(unspecified)',\n",
       "   'page': 0,\n",
       "   'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'context_length': 924,\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'trapped': '',\n",
       "   'author': '(anonymous)',\n",
       "   'format': 'PDF 1.4',\n",
       "   'keywords': '',\n",
       "   'source_file': '01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'title': '(anonymous)',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00'},\n",
       "  'similarity_score': 0.19435656070709229,\n",
       "  'distance': 0.8056434392929077,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_1334ee3b_13',\n",
       "  'content': 'the attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'page': 0,\n",
       "   'subject': '(unspecified)',\n",
       "   'doc_index': 13,\n",
       "   'creator': '(unspecified)',\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'context_length': 950,\n",
       "   'total_pages': 3,\n",
       "   'keywords': '',\n",
       "   'title': '(anonymous)',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'trapped': '',\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'source_file': '03_Advanced_Attention_Variants.pdf',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'author': '(anonymous)'},\n",
       "  'similarity_score': 0.1705533266067505,\n",
       "  'distance': 0.8294466733932495,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrive(\"Sparse Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5018e",
   "metadata": {},
   "source": [
    "### Integration VectorDB Context Pipeline With LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ea9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple rag pipeline using gemini\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# initialize gemini model\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"openai/gpt-oss-20b\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    temperature=0,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "\n",
    "## simple rag functions\n",
    "\n",
    "def rag_simple(query, retriver, llm, top_k=3):\n",
    "    # retrive the context\n",
    "    results = retriver.retrive(query, top_k=top_k)\n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results])\n",
    "\n",
    "    if not context:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    # context + prompt -> llm\n",
    "    prompt = \"\"\"use the following context to answer the question concisely and accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d0330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving document for the query what is a attention mechanism? with top_k=3 and score_threshold=0.0\n",
      "Generating embeddings fro 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7440113743184004a6ed956255281f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(1, 384)\n",
      "retrived 3 documents after filtering\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"what is a attention mechanism?\",rag_retriver,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aa484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attention mechanism is a neural network component that lets the model dynamically weight and focus on different parts of an input sequence when generating an output. Instead of compressing the entire input into a single fixed‑size vector, attention allows the model to “look back” at the full input and decide which elements are most relevant at each step, improving performance on tasks with long or complex sequences.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97713f",
   "metadata": {},
   "source": [
    "### Enhance RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - returns answer, sources, confidence score and optionally the context used for answering the query\n",
    "    \"\"\"\n",
    "\n",
    "    results = retriever.retrive(query, top_k=top_k, score_threshold=min_score)\n",
    "\n",
    "    if not results:\n",
    "        return {'answer': 'No relevent content found', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "\n",
    "    context = \"\\n\\n\".join(doc['content'] for doc in results)\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('sorce', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "\n",
    "    confidence = max(doc['similarity_score'] for doc in results)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely and accurately.\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer: \n",
    "        \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence,\n",
    "        'context': context if return_context else ''\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b46d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving document for the query Who first introduced the concept of attention in machine translation, and when? with top_k=5 and score_threshold=0.2\n",
      "Generating embeddings fro 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538119690e0b44dda73bbf0edf1b90b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(1, 384)\n",
      "retrived 2 documents after filtering\n",
      "answer:  Bahdanau et al. introduced the concept in 2014.\n",
      "sources:  [{'source': '01_Introduction_to_Attention_Mechanisms.pdf', 'page': 0, 'score': 0.2522565722465515, 'preview': 'Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring...'}, {'source': '01_Introduction_to_Attention_Mechanisms.pdf', 'page': 0, 'score': 0.2522565722465515, 'preview': 'Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring...'}]\n",
      "confidence:  0.2522565722465515\n",
      "context: Introduction to Attention Mechanisms\n",
      "A Comprehensive Overview for Deep Learning Practitioners\n",
      "1. What is Attention?\n",
      "Attention mechanisms are a fundamental component of modern deep learning architectures. Inspired\n",
      "by the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\n",
      "attention allows neural networks to dynamically weight different parts of an input sequence when\n",
      "producing an output. Rather than compressing an entire input into a fixed-size vector, attention\n",
      "mechanisms let the model 'look back' at the full input and decide what matters most at each step.\n",
      "The concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\n",
      "they showed that allowing the decoder to attend to different parts of the encoder output dramatically\n",
      "improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\n",
      "\n",
      "Introduction to Attention Mechanisms\n",
      "A Comprehensive Overview for Deep Learning Practitioners\n",
      "1. What is Attention?\n",
      "Attention mechanisms are a fundamental component of modern deep learning architectures. Inspired\n",
      "by the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\n",
      "attention allows neural networks to dynamically weight different parts of an input sequence when\n",
      "producing an output. Rather than compressing an entire input into a fixed-size vector, attention\n",
      "mechanisms let the model 'look back' at the full input and decide what matters most at each step.\n",
      "The concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\n",
      "they showed that allowing the decoder to attend to different parts of the encoder output dramatically\n",
      "improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\n"
     ]
    }
   ],
   "source": [
    "#example usage of the advanced RAG function\n",
    "\n",
    "result=rag_advanced(\"Who first introduced the concept of attention in machine translation, and when?\", rag_retriver, llm, top_k=5, min_score=0.2, return_context=True)\n",
    "print(\"answer: \", result[\"answer\"])\n",
    "print(\"sources: \", result[\"sources\"])\n",
    "print(\"confidence: \",result['confidence'])\n",
    "print(\"context:\",result['context'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb3730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_pipeline)",
   "language": "python",
   "name": "rag_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
