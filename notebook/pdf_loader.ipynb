{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f596c650",
   "metadata": {},
   "source": [
    "### RAG Pipeline -  Data Ingestion To Vector DB pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0b34450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-text-splitters\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b19b76b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 PDF files in ../data\n",
      "\n",
      "processing:1_Final_DFA.pdf\n",
      "loaded 160 pages from the 1_Final_DFA.pdf\n",
      "\n",
      "processing:01_Introduction_to_Attention_Mechanisms.pdf\n",
      "loaded 3 pages from the 01_Introduction_to_Attention_Mechanisms.pdf\n",
      "\n",
      "processing:02_Transformer_and_Self_Attention.pdf\n",
      "loaded 2 pages from the 02_Transformer_and_Self_Attention.pdf\n",
      "\n",
      "processing:03_Advanced_Attention_Variants.pdf\n",
      "loaded 3 pages from the 03_Advanced_Attention_Variants.pdf\n",
      "\n",
      "total documents loaded: 168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_all_pdfs(pdf_directory):\n",
    "    all_documents=[]\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "\n",
    "    for pdf in pdf_files:\n",
    "        print(f\"\\nprocessing:{pdf.name}\")\n",
    "        try:\n",
    "            loader=PyMuPDFLoader(str(pdf))\n",
    "            documents=loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source_file\"]=pdf.name\n",
    "                doc.metadata[\"file_type\"]=\"pdf\"\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"loaded {len(documents)} pages from the {pdf.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error loading {pdf.name}: {e}\")\n",
    "\n",
    "    print(f\"\\ntotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents=load_all_pdfs(\"../data\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71be1119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\\nfor the Transformer architecture and the modern era of large language models.\\n2. The Core Intuition\\nThink of attention like a search engine. You have a query (what you're looking for), a set of keys (labels\\nor identifiers for stored information), and values (the actual information stored). The attention\\nmechanism computes a similarity score between the query and each key, converts these scores into\\nprobabilities using softmax, and returns a weighted sum of the values.\\nThe general attention formula is:\\nAttention(Q, K, V) = softmax(QKT / sqrt(d_k)) * V\\nWhere Q is the query matrix, K is the key matrix, V is the value matrix, and d_k is the dimension of the\\nkeys. The division by sqrt(d_k) prevents the dot products from growing too large in magnitude, which\\nwould push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\\nSoft attention computes a weighted average over all input positions — it is fully differentiable and can\\nbe trained end-to-end with backpropagation. Hard attention, on the other hand, selects a single input\\nposition at each step stochastically. While hard attention can be more efficient, it requires reinforcement\\nlearning techniques to train since it is not differentiable.\\n3.2 Self-Attention (Intra-Attention)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='Self-attention allows a sequence to attend to itself. Every token in the input sequence computes\\nattention scores with every other token, capturing relationships within the same sequence. This is\\nextremely powerful for tasks like understanding pronoun reference, long-range dependencies, and\\nsyntactic structure. Self-attention is the cornerstone of the Transformer architecture.\\n3.3 Cross-Attention\\nCross-attention is used when queries come from one sequence (e.g., the decoder) and keys/values\\ncome from another sequence (e.g., the encoder output). This is the classic encoder-decoder attention\\nused in machine translation and image captioning tasks.\\n3.4 Multi-Head Attention\\nInstead of performing a single attention function, multi-head attention runs h parallel attention\\noperations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation\\nsubspaces at different positions.\\nMultiHead(Q,K,V) = Concat(head_1, ..., head_h) * W_O\\nwhere head_i = Attention(Q*W_Q_i, K*W_K_i, V*W_V_i)\\n4. Why Attention Changed Everything\\nBefore attention, sequence-to-sequence models used RNNs and LSTMs which suffered from the\\nvanishing gradient problem and struggled to capture long-range dependencies. Attention solved this by\\nproviding direct connections between any two positions in the sequence, regardless of distance. This\\nmade training faster, more parallelizable (unlike RNNs which are sequential), and more interpretable\\nsince we can visualize which tokens the model attends to.\\n5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field\\nAny token can directly attend to any other token\\nInterpretability\\nAttention weights provide insight into model decisions\\nPermutation Equivariant\\nNo built-in notion of order (positional encoding needed)\\nQuadratic Complexity\\nO(n^2) cost w.r.t. sequence length — a known limitation\\n6. Summary\\nAttention mechanisms revolutionized deep learning by enabling models to dynamically focus on\\nrelevant information. From the original Bahdanau attention to modern multi-head self-attention, these\\ntechniques power state-of-the-art NLP, vision, and multimodal systems. Understanding attention is'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='essential for anyone working with Transformers, BERT, GPT, or any modern AI system.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"Transformer Architecture & Self-Attention\\nDeep Dive into 'Attention is All You Need'\\n1. The Transformer Model\\nThe Transformer, introduced by Vaswani et al. in 2017 in the landmark paper 'Attention is All You\\nNeed', completely replaced recurrence and convolutions with attention mechanisms. It consists of an\\nencoder stack and a decoder stack, each made up of identical layers. This architecture became the\\nfoundation for BERT, GPT, T5, and virtually every state-of-the-art NLP model today.\\n2. Encoder Architecture\\nThe encoder is composed of N identical layers (typically N=6). Each layer has two sub-layers: a\\nmulti-head self-attention mechanism, and a position-wise fully connected feed-forward network. Each\\nsub-layer has a residual connection followed by layer normalization.\\nLayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\\npassing through the attention or FFN transformations, making it much easier to train deep networks.\\n3. Positional Encoding\\nSince self-attention has no inherent notion of order (it's permutation equivariant), positional encodings\\nare added to the input embeddings to inject information about token positions. The original Transformer\\nused sinusoidal positional encodings:\\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\nSinusoidal encodings have the nice property that the encoding for position pos+k can be expressed as\\na linear function of the encoding at pos, which may help the model generalize to unseen sequence\\nlengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\\nHere's exactly how scaled dot-product attention works:\\nStep\\nOperation\\nShape\\n1\\nProject input X into Q, K, V matrices\\n(seq_len, d_model) → (seq_len, d_k)\\n2\\nCompute dot product: Q × K^T\\n(seq_len, seq_len)\\n3\\nScale by 1/sqrt(d_k)\\n(seq_len, seq_len)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"4\\nApply optional mask (for decoder)\\n(seq_len, seq_len)\\n5\\nApply softmax row-wise\\n(seq_len, seq_len) — rows sum to 1\\n6\\nMultiply by V\\n(seq_len, d_v)\\n5. The Decoder and Masked Attention\\nThe decoder has three sub-layers per block: a masked multi-head self-attention layer, a cross-attention\\nlayer (attending to the encoder output), and a feed-forward network. The masking in the first sub-layer\\nprevents the decoder from attending to future positions — this is critical for autoregressive generation\\nwhere the model must predict one token at a time without 'cheating' by looking at future tokens.\\n6. Feed-Forward Networks in Transformers\\nEach Transformer layer contains a position-wise FFN applied independently to each position:\\nFFN(x) = max(0, x*W_1 + b_1) * W_2 + b_2\\nTypically the inner dimension d_ff = 4 * d_model (e.g., 2048 when d_model=512). This FFN is where\\nmuch of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\\n7. Transformer Hyperparameters (Original Paper)\\nHyperparameter\\nBase Model\\nLarge Model\\nd_model (embedding dim)\\n512\\n1024\\nNumber of layers (N)\\n6\\n6\\nNumber of heads (h)\\n8\\n16\\nd_k = d_v\\n64\\n64\\nd_ff (FFN inner dim)\\n2048\\n4096\\nDropout rate\\n0.1\\n0.3\\nParameters\\n65M\\n213M\\n8. Why Transformers Beat RNNs\\nRNNs process sequences token by token, making parallelization impossible during training. LSTMs\\nmitigate vanishing gradients but still struggle with very long sequences. Transformers process all\\ntokens simultaneously, support full parallelism on GPUs/TPUs, handle long-range dependencies with\\nO(1) path length between any two positions, and scale remarkably well with data and compute —\\nleading to the era of large language models.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)\\nThis allows the attention computation to be rewritten using matrix associativity, computing KV\\naggregations first and then applying Q — avoiding the n×n attention matrix entirely. Examples include\\nPerformer (using random Fourier features) and Linear Transformer.\\n4. Flash Attention'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Flash Attention (Dao et al., 2022) takes a different approach — instead of approximating attention, it\\ncomputes exact attention but uses a hardware-aware algorithm to minimize memory I/O. The key\\ninsight is that the bottleneck in standard attention is not compute but memory bandwidth between HBM\\n(high-bandwidth memory) and SRAM (fast on-chip memory).\\nFlash Attention uses tiling to split Q, K, V into blocks and processes them in SRAM, never materializing\\nthe full n×n attention matrix in HBM. This achieves 2-4x speedup and O(n) memory usage while\\ncomputing mathematically identical results to standard attention. Flash Attention 2 and 3 further\\nimproved performance with better parallelism strategies.\\n5. Multi-Query and Grouped-Query Attention\\n5.1 Multi-Query Attention (MQA)\\nMulti-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —\\ncritical for autoregressive generation where cached keys and values consume large amounts of\\nmemory. MQA is used in models like PaLM and Falcon.\\n5.2 Grouped-Query Attention (GQA)\\nGrouped-Query Attention (Ainslie et al., 2023) is a middle ground between MHA and MQA. Query\\nheads are divided into G groups, and each group shares one key and value head. GQA achieves\\nquality close to MHA with inference speed close to MQA. It is used in Llama 2, Llama 3, Mistral, and\\nmany modern open-source LLMs.\\n6. Relative Positional Encodings\\n6.1 Rotary Position Embedding (RoPE)\\nRoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance\\n(m-n), giving the model a natural sense of relative position. RoPE generalizes well to longer sequences\\nthan seen during training and is used in GPT-NeoX, LLaMA, Falcon, and most modern LLMs.\\n6.2 ALiBi (Attention with Linear Biases)\\nALiBi (Press et al., 2021) adds a linear bias to attention scores based on the distance between query\\nand key positions: score(q_i, k_j) -= m * |i - j|, where m is a head-specific slope. This penalizes\\nattending to distant tokens, encouraging locality. ALiBi models extrapolate well to longer sequences\\nwithout any retraining.\\n7. Comparison of Attention Variants'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Method\\nComplexity\\nExact?\\nUse Case\\nStandard MHA\\nO(n^2)\\nYes\\nGeneral purpose, short sequences\\nLongformer\\nO(n * w)\\nApprox\\nLong documents\\nBigBird\\nO(n)\\nApprox\\nVery long sequences\\nFlash Attention\\nO(n^2) compute, O(n) mem\\nYes\\nGPU-efficient training\\nLinear Attention\\nO(n)\\nApprox\\nExtreme length, efficiency\\nMQA / GQA\\nO(n^2)\\nYes\\nFast inference, small KV cache\\n8. Summary & Takeaways\\nThe field of attention mechanisms has evolved rapidly since 2017. Standard multi-head attention\\nremains the gold standard for quality, while efficient variants like Flash Attention, GQA, and sparse\\nattention methods make it practical to scale to longer sequences and larger models. Choosing the right\\nattention variant depends on your sequence length, hardware constraints, inference requirements, and\\nacceptable quality trade-offs. Modern LLMs like Llama 3 and Mistral combine Flash Attention + GQA +\\nRoPE for an optimal balance of quality, speed, and memory efficiency.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "438eb97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 8 documents into 19 chunks\n",
      "\n",
      " example chunk:\n",
      "content: Introduction to Attention Mechanisms\n",
      "A Comprehensive Overview for Deep Learning Practitioners\n",
      "1. What is Attention?\n",
      "Attention mechanisms are a fundamental component of modern deep learning architectur\n",
      "metadata: {'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\\nfor the Transformer architecture and the modern era of large language models.\\n2. The Core Intuition\\nThink of attention like a search engine. You have a query (what you're looking for), a set of keys (labels\\nor identifiers for stored information), and values (the actual information stored). The attention\\nmechanism computes a similarity score between the query and each key, converts these scores into\\nprobabilities using softmax, and returns a weighted sum of the values.\\nThe general attention formula is:\\nAttention(Q, K, V) = softmax(QKT / sqrt(d_k)) * V\\nWhere Q is the query matrix, K is the key matrix, V is the value matrix, and d_k is the dimension of the\\nkeys. The division by sqrt(d_k) prevents the dot products from growing too large in magnitude, which\\nwould push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='would push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\\nSoft attention computes a weighted average over all input positions — it is fully differentiable and can\\nbe trained end-to-end with backpropagation. Hard attention, on the other hand, selects a single input\\nposition at each step stochastically. While hard attention can be more efficient, it requires reinforcement\\nlearning techniques to train since it is not differentiable.\\n3.2 Self-Attention (Intra-Attention)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='Self-attention allows a sequence to attend to itself. Every token in the input sequence computes\\nattention scores with every other token, capturing relationships within the same sequence. This is\\nextremely powerful for tasks like understanding pronoun reference, long-range dependencies, and\\nsyntactic structure. Self-attention is the cornerstone of the Transformer architecture.\\n3.3 Cross-Attention\\nCross-attention is used when queries come from one sequence (e.g., the decoder) and keys/values\\ncome from another sequence (e.g., the encoder output). This is the classic encoder-decoder attention\\nused in machine translation and image captioning tasks.\\n3.4 Multi-Head Attention\\nInstead of performing a single attention function, multi-head attention runs h parallel attention\\noperations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='operations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation\\nsubspaces at different positions.\\nMultiHead(Q,K,V) = Concat(head_1, ..., head_h) * W_O\\nwhere head_i = Attention(Q*W_Q_i, K*W_K_i, V*W_V_i)\\n4. Why Attention Changed Everything\\nBefore attention, sequence-to-sequence models used RNNs and LSTMs which suffered from the\\nvanishing gradient problem and struggled to capture long-range dependencies. Attention solved this by\\nproviding direct connections between any two positions in the sequence, regardless of distance. This\\nmade training faster, more parallelizable (unlike RNNs which are sequential), and more interpretable\\nsince we can visualize which tokens the model attends to.\\n5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field\\nAny token can directly attend to any other token\\nInterpretability\\nAttention weights provide insight into model decisions\\nPermutation Equivariant\\nNo built-in notion of order (positional encoding needed)\\nQuadratic Complexity\\nO(n^2) cost w.r.t. sequence length — a known limitation\\n6. Summary\\nAttention mechanisms revolutionized deep learning by enabling models to dynamically focus on\\nrelevant information. From the original Bahdanau attention to modern multi-head self-attention, these\\ntechniques power state-of-the-art NLP, vision, and multimodal systems. Understanding attention is'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='essential for anyone working with Transformers, BERT, GPT, or any modern AI system.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"Transformer Architecture & Self-Attention\\nDeep Dive into 'Attention is All You Need'\\n1. The Transformer Model\\nThe Transformer, introduced by Vaswani et al. in 2017 in the landmark paper 'Attention is All You\\nNeed', completely replaced recurrence and convolutions with attention mechanisms. It consists of an\\nencoder stack and a decoder stack, each made up of identical layers. This architecture became the\\nfoundation for BERT, GPT, T5, and virtually every state-of-the-art NLP model today.\\n2. Encoder Architecture\\nThe encoder is composed of N identical layers (typically N=6). Each layer has two sub-layers: a\\nmulti-head self-attention mechanism, and a position-wise fully connected feed-forward network. Each\\nsub-layer has a residual connection followed by layer normalization.\\nLayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"LayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\\npassing through the attention or FFN transformations, making it much easier to train deep networks.\\n3. Positional Encoding\\nSince self-attention has no inherent notion of order (it's permutation equivariant), positional encodings\\nare added to the input embeddings to inject information about token positions. The original Transformer\\nused sinusoidal positional encodings:\\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\nSinusoidal encodings have the nice property that the encoding for position pos+k can be expressed as\\na linear function of the encoding at pos, which may help the model generalize to unseen sequence\\nlengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"lengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\\nHere's exactly how scaled dot-product attention works:\\nStep\\nOperation\\nShape\\n1\\nProject input X into Q, K, V matrices\\n(seq_len, d_model) → (seq_len, d_k)\\n2\\nCompute dot product: Q × K^T\\n(seq_len, seq_len)\\n3\\nScale by 1/sqrt(d_k)\\n(seq_len, seq_len)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"4\\nApply optional mask (for decoder)\\n(seq_len, seq_len)\\n5\\nApply softmax row-wise\\n(seq_len, seq_len) — rows sum to 1\\n6\\nMultiply by V\\n(seq_len, d_v)\\n5. The Decoder and Masked Attention\\nThe decoder has three sub-layers per block: a masked multi-head self-attention layer, a cross-attention\\nlayer (attending to the encoder output), and a feed-forward network. The masking in the first sub-layer\\nprevents the decoder from attending to future positions — this is critical for autoregressive generation\\nwhere the model must predict one token at a time without 'cheating' by looking at future tokens.\\n6. Feed-Forward Networks in Transformers\\nEach Transformer layer contains a position-wise FFN applied independently to each position:\\nFFN(x) = max(0, x*W_1 + b_1) * W_2 + b_2\\nTypically the inner dimension d_ff = 4 * d_model (e.g., 2048 when d_model=512). This FFN is where\\nmuch of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"much of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\\n7. Transformer Hyperparameters (Original Paper)\\nHyperparameter\\nBase Model\\nLarge Model\\nd_model (embedding dim)\\n512\\n1024\\nNumber of layers (N)\\n6\\n6\\nNumber of heads (h)\\n8\\n16\\nd_k = d_v\\n64\\n64\\nd_ff (FFN inner dim)\\n2048\\n4096\\nDropout rate\\n0.1\\n0.3\\nParameters\\n65M\\n213M\\n8. Why Transformers Beat RNNs\\nRNNs process sequences token by token, making parallelization impossible during training. LSTMs\\nmitigate vanishing gradients but still struggle with very long sequences. Transformers process all\\ntokens simultaneously, support full parallelism on GPUs/TPUs, handle long-range dependencies with\\nO(1) path length between any two positions, and scale remarkably well with data and compute —\\nleading to the era of large language models.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='the attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='idea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)\\nThis allows the attention computation to be rewritten using matrix associativity, computing KV\\naggregations first and then applying Q — avoiding the n×n attention matrix entirely. Examples include\\nPerformer (using random Fourier features) and Linear Transformer.\\n4. Flash Attention'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Flash Attention (Dao et al., 2022) takes a different approach — instead of approximating attention, it\\ncomputes exact attention but uses a hardware-aware algorithm to minimize memory I/O. The key\\ninsight is that the bottleneck in standard attention is not compute but memory bandwidth between HBM\\n(high-bandwidth memory) and SRAM (fast on-chip memory).\\nFlash Attention uses tiling to split Q, K, V into blocks and processes them in SRAM, never materializing\\nthe full n×n attention matrix in HBM. This achieves 2-4x speedup and O(n) memory usage while\\ncomputing mathematically identical results to standard attention. Flash Attention 2 and 3 further\\nimproved performance with better parallelism strategies.\\n5. Multi-Query and Grouped-Query Attention\\n5.1 Multi-Query Attention (MQA)\\nMulti-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Multi-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —\\ncritical for autoregressive generation where cached keys and values consume large amounts of\\nmemory. MQA is used in models like PaLM and Falcon.\\n5.2 Grouped-Query Attention (GQA)\\nGrouped-Query Attention (Ainslie et al., 2023) is a middle ground between MHA and MQA. Query\\nheads are divided into G groups, and each group shares one key and value head. GQA achieves\\nquality close to MHA with inference speed close to MQA. It is used in Llama 2, Llama 3, Mistral, and\\nmany modern open-source LLMs.\\n6. Relative Positional Encodings\\n6.1 Rotary Position Embedding (RoPE)\\nRoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='RoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance\\n(m-n), giving the model a natural sense of relative position. RoPE generalizes well to longer sequences\\nthan seen during training and is used in GPT-NeoX, LLaMA, Falcon, and most modern LLMs.\\n6.2 ALiBi (Attention with Linear Biases)\\nALiBi (Press et al., 2021) adds a linear bias to attention scores based on the distance between query\\nand key positions: score(q_i, k_j) -= m * |i - j|, where m is a head-specific slope. This penalizes\\nattending to distant tokens, encouraging locality. ALiBi models extrapolate well to longer sequences\\nwithout any retraining.\\n7. Comparison of Attention Variants'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Method\\nComplexity\\nExact?\\nUse Case\\nStandard MHA\\nO(n^2)\\nYes\\nGeneral purpose, short sequences\\nLongformer\\nO(n * w)\\nApprox\\nLong documents\\nBigBird\\nO(n)\\nApprox\\nVery long sequences\\nFlash Attention\\nO(n^2) compute, O(n) mem\\nYes\\nGPU-efficient training\\nLinear Attention\\nO(n)\\nApprox\\nExtreme length, efficiency\\nMQA / GQA\\nO(n^2)\\nYes\\nFast inference, small KV cache\\n8. Summary & Takeaways\\nThe field of attention mechanisms has evolved rapidly since 2017. Standard multi-head attention\\nremains the gold standard for quality, while efficient variants like Flash Attention, GQA, and sparse\\nattention methods make it practical to scale to longer sequences and larger models. Choosing the right\\nattention variant depends on your sequence length, hardware constraints, inference requirements, and\\nacceptable quality trade-offs. Modern LLMs like Llama 3 and Mistral combine Flash Attention + GQA +\\nRoPE for an optimal balance of quality, speed, and memory efficiency.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### text splitting get into chunks\n",
    "\n",
    "from langchain_core import documents\n",
    "\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200): # what is a chunk overlap - ans - it is the number of characters that will be repeated in the next chunk to provide context\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len, # this is the function that will be used to calculate the length of the text, in this case we are using the built-in len function which counts the number of characters in the text. This is important because we want to split the text into chunks of a certain size, and we need to know how long the text is to do that.\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"] #  these are the separators that will be used to split the text. The text splitter will try to split the text using these separators in order. So it will first try to split the text using double newlines, then single newlines, then spaces, and finally if it can't split the text using any of those separators, it will split the text at the chunk size regardless of the separator. This is important because we want to try to split the text at natural break points (like paragraphs or sentences) before splitting it at arbitrary points (like in the middle of a word).\n",
    "    )\n",
    "\n",
    "    split_docs=text_splitter.split_documents(all_pdf_documents)\n",
    "    print(f\"split {len(all_pdf_documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(\"\\n example chunk:\")\n",
    "        print(f\"content: {split_docs[0].page_content[:200]}\")\n",
    "        print(f\"metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "\n",
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bdf5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb3bfbc",
   "metadata": {},
   "source": [
    "### Embedding & Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2d084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec499994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "from typing  import List , Dict , Any , Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebf15bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model:all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38263b3441545548315a4d05c0221dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension:384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2340f67ba10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentance embeddings\n",
    "        \"\"\"\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model() # its a protected function because we don't want the user to call it directly, we want them to use the get_embedding function which will call this function if the model is not already loaded.\n",
    "\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the SentenceTransformer model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model:{self.model_name}\")\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension:{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading model {self.model_name}:{e}\")\n",
    "            raise e\n",
    "        \n",
    "    def generate_embeddings(self,texts:List[str])->np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "        texts:list of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddins with shape (len(texts),embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Call _load_model() to load the model before generating embeddings.\")\n",
    "        \n",
    "        print(f\"Generating embeddings fro {len(texts)} texts\")\n",
    "        embeddings=self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embedings with stage:{embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b4092",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7006c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store initialize. collection:pdf_documents\n",
      "Existing documents in collection:555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2340f67bcb0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages a vector store using ChromaDB for storing and retrieving document embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self,collection_name:str =\"pdf_documents\",persist_directory: str =\"../data/vector_store\"):\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"vector store initialize. collection:{self.collection_name}\")\n",
    "            print(f\"Existing documents in collection:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        \n",
    "        \"\"\"\n",
    "        add documents and their embedings tp the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: list of documents to add to the store. Each document should have a unique ID in its metadata under the key \"id\".\n",
    "            embeddings: numpy array of embeddings corresponding to the documents, with shape (len(documents), embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if(len(documents)!=len(embeddings)):\n",
    "            raise ValueError(\"number of documents and embeddings must be the same\" )\n",
    "\n",
    "        print(f\"adding {len(documents)} document to vector store...\")\n",
    "\n",
    "        #prepare data for chroma db    \n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_text=[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "\n",
    "            #generate id\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\" # generate a unique id for the document using uuid and the index of the document in the list. This ensures that even if there are duplicate documents, they will have unique IDs in the vector store.\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #generate metadata\n",
    "            metadata=dict(doc.metadata) # make a copy of the document metadata to avoid modifying the original document's metadata\n",
    "            metadata['doc_index']=i # add the index of the document in the original list to the metadata. This can be useful for debugging and for retrieving the original document later if needed.\n",
    "            metadata['context_length']=len(doc.page_content) # add the length of the document text to the metadata. This can be useful for filtering documents based on their length during retrieval.  \n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #Document content\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #embeddings\n",
    "            embeddings_list.append(embedding.tolist()) # convert the embedding from a numpy array to a list so that it can be stored in ChromaDB, which expects embeddings to be in list format.\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"succesfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"total documnets in collection:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error adding documents to vector store: {e}\")\n",
    "            raise e \n",
    "        \n",
    "\n",
    "vector_store=VectorStore()\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eb1625b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content=\"improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\\nfor the Transformer architecture and the modern era of large language models.\\n2. The Core Intuition\\nThink of attention like a search engine. You have a query (what you're looking for), a set of keys (labels\\nor identifiers for stored information), and values (the actual information stored). The attention\\nmechanism computes a similarity score between the query and each key, converts these scores into\\nprobabilities using softmax, and returns a weighted sum of the values.\\nThe general attention formula is:\\nAttention(Q, K, V) = softmax(QKT / sqrt(d_k)) * V\\nWhere Q is the query matrix, K is the key matrix, V is the value matrix, and d_k is the dimension of the\\nkeys. The division by sqrt(d_k) prevents the dot products from growing too large in magnitude, which\\nwould push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='would push the softmax into regions with very small gradients.\\n3. Types of Attention\\n3.1 Soft vs Hard Attention\\nSoft attention computes a weighted average over all input positions — it is fully differentiable and can\\nbe trained end-to-end with backpropagation. Hard attention, on the other hand, selects a single input\\nposition at each step stochastically. While hard attention can be more efficient, it requires reinforcement\\nlearning techniques to train since it is not differentiable.\\n3.2 Self-Attention (Intra-Attention)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='Self-attention allows a sequence to attend to itself. Every token in the input sequence computes\\nattention scores with every other token, capturing relationships within the same sequence. This is\\nextremely powerful for tasks like understanding pronoun reference, long-range dependencies, and\\nsyntactic structure. Self-attention is the cornerstone of the Transformer architecture.\\n3.3 Cross-Attention\\nCross-attention is used when queries come from one sequence (e.g., the decoder) and keys/values\\ncome from another sequence (e.g., the encoder output). This is the classic encoder-decoder attention\\nused in machine translation and image captioning tasks.\\n3.4 Multi-Head Attention\\nInstead of performing a single attention function, multi-head attention runs h parallel attention\\noperations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='operations (heads) with different learned projections. The outputs are concatenated and projected\\nagain. This allows the model to simultaneously attend to information from different representation\\nsubspaces at different positions.\\nMultiHead(Q,K,V) = Concat(head_1, ..., head_h) * W_O\\nwhere head_i = Attention(Q*W_Q_i, K*W_K_i, V*W_V_i)\\n4. Why Attention Changed Everything\\nBefore attention, sequence-to-sequence models used RNNs and LSTMs which suffered from the\\nvanishing gradient problem and struggled to capture long-range dependencies. Attention solved this by\\nproviding direct connections between any two positions in the sequence, regardless of distance. This\\nmade training faster, more parallelizable (unlike RNNs which are sequential), and more interpretable\\nsince we can visualize which tokens the model attends to.\\n5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='5. Key Properties of Attention\\nProperty\\nDescription\\nParallelism\\nAll attention scores computed simultaneously, unlike RNNs\\nGlobal Receptive Field\\nAny token can directly attend to any other token\\nInterpretability\\nAttention weights provide insight into model decisions\\nPermutation Equivariant\\nNo built-in notion of order (positional encoding needed)\\nQuadratic Complexity\\nO(n^2) cost w.r.t. sequence length — a known limitation\\n6. Summary\\nAttention mechanisms revolutionized deep learning by enabling models to dynamically focus on\\nrelevant information. From the original Bahdanau attention to modern multi-head self-attention, these\\ntechniques power state-of-the-art NLP, vision, and multimodal systems. Understanding attention is'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '01_Introduction_to_Attention_Mechanisms.pdf', 'file_type': 'pdf'}, page_content='essential for anyone working with Transformers, BERT, GPT, or any modern AI system.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"Transformer Architecture & Self-Attention\\nDeep Dive into 'Attention is All You Need'\\n1. The Transformer Model\\nThe Transformer, introduced by Vaswani et al. in 2017 in the landmark paper 'Attention is All You\\nNeed', completely replaced recurrence and convolutions with attention mechanisms. It consists of an\\nencoder stack and a decoder stack, each made up of identical layers. This architecture became the\\nfoundation for BERT, GPT, T5, and virtually every state-of-the-art NLP model today.\\n2. Encoder Architecture\\nThe encoder is composed of N identical layers (typically N=6). Each layer has two sub-layers: a\\nmulti-head self-attention mechanism, and a position-wise fully connected feed-forward network. Each\\nsub-layer has a residual connection followed by layer normalization.\\nLayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"LayerOutput = LayerNorm(x + Sublayer(x))\\nThe residual connections are crucial — they allow gradients to flow directly through the network without\\npassing through the attention or FFN transformations, making it much easier to train deep networks.\\n3. Positional Encoding\\nSince self-attention has no inherent notion of order (it's permutation equivariant), positional encodings\\nare added to the input embeddings to inject information about token positions. The original Transformer\\nused sinusoidal positional encodings:\\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\nSinusoidal encodings have the nice property that the encoding for position pos+k can be expressed as\\na linear function of the encoding at pos, which may help the model generalize to unseen sequence\\nlengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"lengths. Many modern models replace these with learned positional embeddings or relative positional\\nencodings (RoPE, ALiBi).\\n4. Scaled Dot-Product Attention — Step by Step\\nHere's exactly how scaled dot-product attention works:\\nStep\\nOperation\\nShape\\n1\\nProject input X into Q, K, V matrices\\n(seq_len, d_model) → (seq_len, d_k)\\n2\\nCompute dot product: Q × K^T\\n(seq_len, seq_len)\\n3\\nScale by 1/sqrt(d_k)\\n(seq_len, seq_len)\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"4\\nApply optional mask (for decoder)\\n(seq_len, seq_len)\\n5\\nApply softmax row-wise\\n(seq_len, seq_len) — rows sum to 1\\n6\\nMultiply by V\\n(seq_len, d_v)\\n5. The Decoder and Masked Attention\\nThe decoder has three sub-layers per block: a masked multi-head self-attention layer, a cross-attention\\nlayer (attending to the encoder output), and a feed-forward network. The masking in the first sub-layer\\nprevents the decoder from attending to future positions — this is critical for autoregressive generation\\nwhere the model must predict one token at a time without 'cheating' by looking at future tokens.\\n6. Feed-Forward Networks in Transformers\\nEach Transformer layer contains a position-wise FFN applied independently to each position:\\nFFN(x) = max(0, x*W_1 + b_1) * W_2 + b_2\\nTypically the inner dimension d_ff = 4 * d_model (e.g., 2048 when d_model=512). This FFN is where\\nmuch of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\02_Transformer_and_Self_Attention.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '02_Transformer_and_Self_Attention.pdf', 'file_type': 'pdf'}, page_content=\"much of the model's 'knowledge' is believed to be stored. Some research suggests FFN layers act as\\nkey-value memories.\\n7. Transformer Hyperparameters (Original Paper)\\nHyperparameter\\nBase Model\\nLarge Model\\nd_model (embedding dim)\\n512\\n1024\\nNumber of layers (N)\\n6\\n6\\nNumber of heads (h)\\n8\\n16\\nd_k = d_v\\n64\\n64\\nd_ff (FFN inner dim)\\n2048\\n4096\\nDropout rate\\n0.1\\n0.3\\nParameters\\n65M\\n213M\\n8. Why Transformers Beat RNNs\\nRNNs process sequences token by token, making parallelization impossible during training. LSTMs\\nmitigate vanishing gradients but still struggle with very long sequences. Transformers process all\\ntokens simultaneously, support full parallelism on GPUs/TPUs, handle long-range dependencies with\\nO(1) path length between any two positions, and scale remarkably well with data and compute —\\nleading to the era of large language models.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='the attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 0, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='idea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)\\nThis allows the attention computation to be rewritten using matrix associativity, computing KV\\naggregations first and then applying Q — avoiding the n×n attention matrix entirely. Examples include\\nPerformer (using random Fourier features) and Linear Transformer.\\n4. Flash Attention'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Flash Attention (Dao et al., 2022) takes a different approach — instead of approximating attention, it\\ncomputes exact attention but uses a hardware-aware algorithm to minimize memory I/O. The key\\ninsight is that the bottleneck in standard attention is not compute but memory bandwidth between HBM\\n(high-bandwidth memory) and SRAM (fast on-chip memory).\\nFlash Attention uses tiling to split Q, K, V into blocks and processes them in SRAM, never materializing\\nthe full n×n attention matrix in HBM. This achieves 2-4x speedup and O(n) memory usage while\\ncomputing mathematically identical results to standard attention. Flash Attention 2 and 3 further\\nimproved performance with better parallelism strategies.\\n5. Multi-Query and Grouped-Query Attention\\n5.1 Multi-Query Attention (MQA)\\nMulti-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Multi-Query Attention (Shazeer, 2019) uses multiple query heads but shares a single key and value\\nhead across all query heads. This dramatically reduces the size of the KV cache during inference —\\ncritical for autoregressive generation where cached keys and values consume large amounts of\\nmemory. MQA is used in models like PaLM and Falcon.\\n5.2 Grouped-Query Attention (GQA)\\nGrouped-Query Attention (Ainslie et al., 2023) is a middle ground between MHA and MQA. Query\\nheads are divided into G groups, and each group shares one key and value head. GQA achieves\\nquality close to MHA with inference speed close to MQA. It is used in Llama 2, Llama 3, Mistral, and\\nmany modern open-source LLMs.\\n6. Relative Positional Encodings\\n6.1 Rotary Position Embedding (RoPE)\\nRoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 1, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='RoPE (Su et al., 2021) encodes position by rotating query and key vectors in 2D planes. The dot\\nproduct between a query at position m and a key at position n depends only on their relative distance\\n(m-n), giving the model a natural sense of relative position. RoPE generalizes well to longer sequences\\nthan seen during training and is used in GPT-NeoX, LLaMA, Falcon, and most modern LLMs.\\n6.2 ALiBi (Attention with Linear Biases)\\nALiBi (Press et al., 2021) adds a linear bias to attention scores based on the distance between query\\nand key positions: score(q_i, k_j) -= m * |i - j|, where m is a head-specific slope. This penalizes\\nattending to distant tokens, encouraging locality. ALiBi models extrapolate well to longer sequences\\nwithout any retraining.\\n7. Comparison of Attention Variants'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-02-23T11:04:25+00:00', 'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': '(anonymous)', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2026-02-23T11:04:25+00:00', 'trapped': '', 'modDate': \"D:20260223110425+00'00'\", 'creationDate': \"D:20260223110425+00'00'\", 'page': 2, 'source_file': '03_Advanced_Attention_Variants.pdf', 'file_type': 'pdf'}, page_content='Method\\nComplexity\\nExact?\\nUse Case\\nStandard MHA\\nO(n^2)\\nYes\\nGeneral purpose, short sequences\\nLongformer\\nO(n * w)\\nApprox\\nLong documents\\nBigBird\\nO(n)\\nApprox\\nVery long sequences\\nFlash Attention\\nO(n^2) compute, O(n) mem\\nYes\\nGPU-efficient training\\nLinear Attention\\nO(n)\\nApprox\\nExtreme length, efficiency\\nMQA / GQA\\nO(n^2)\\nYes\\nFast inference, small KV cache\\n8. Summary & Takeaways\\nThe field of attention mechanisms has evolved rapidly since 2017. Standard multi-head attention\\nremains the gold standard for quality, while efficient variants like Flash Attention, GQA, and sparse\\nattention methods make it practical to scale to longer sequences and larger models. Choosing the right\\nattention variant depends on your sequence length, hardware constraints, inference requirements, and\\nacceptable quality trade-offs. Modern LLMs like Llama 3 and Mistral combine Flash Attention + GQA +\\nRoPE for an optimal balance of quality, speed, and memory efficiency.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aa574",
   "metadata": {},
   "source": [
    "## convert the text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dadacc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings fro 19 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a6f29833b2410e988953f4734a04c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(19, 384)\n",
      "adding 19 document to vector store...\n",
      "succesfully added 19 documents to vector store\n",
      "total documnets in collection:574\n"
     ]
    }
   ],
   "source": [
    "# extraxt the text\n",
    "texts=[doc.page_content for doc in chunks] # extract the text content from each document chunk to create a list of strings that can be passed to the embedding model for generating embeddings. Each element in the \"texts\" list corresponds to the text content of a document chunk, which will be embedded and stored in the vector store for later retrieval during RAG operations.\n",
    "\n",
    "# generate the embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# store in the vecor db\n",
    "vector_store.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0eee3",
   "metadata": {},
   "source": [
    "### Retrival Pipeline From Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6771bdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetrival at 0x2340f67bb60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetrival:\n",
    "    \n",
    "    def __init__(self,vector_store:VectorStore,embedding_manager:EmbeddingManager):\n",
    "        \"\"\"Initailize the retriver\n",
    "        Args:\n",
    "        vector_store: instance of the VectorStore class that manages the vector database for storing and retrieving document embeddings.\n",
    "        embedding_manager: instance of the EmbeddingManager class that handles generating embeddings for query texts.\n",
    "        \"\"\"\n",
    "        self.vector_store=vector_store\n",
    "        self.embedding_manager=embedding_manager \n",
    "\n",
    "    def retrive(self,query:str,top_k:int=5,score_threshold:float=0.0)-> List[Dict[str,Any]]:\n",
    "        \"\"\"\n",
    "        retrive relevent dpcument for a  query\n",
    "        \n",
    "        Args:\n",
    "        query; the input query comes form the user that we want to find relevant documents for. This is typically a natural language question or statement that the user inputs to the RAG system.\n",
    "        top_k; top k results to return. This parameter controls how many of the most relevant documents will be returned by the retriever. A higher value of top_k will return more documents, but may also include less relevant ones, while a lower value will return fewer but more relevant documents.\n",
    "        score_threashold: the minimum cosine similarity score for a document to be considered relevant and included in the results. This parameter helps filter out documents that are not sufficiently similar to the query, ensuring that only documents with a cosine similarity score above this threshold are returned in the results.\n",
    "\n",
    "        Returns:\n",
    "        A list of dictionaries, where each dictionary contains the retrieved document's text, metadata, and its cosine similarity score with respect to the query. The list is sorted in descending order of relevance (highest cosine similarity score first).\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"retriving document for the query {query} with top_k={top_k} and score_threshold={score_threshold}\")\n",
    "\n",
    "        #generate embedding for the query\n",
    "        query_embedding=self.embedding_manager.generate_embeddings([query])[0] # generate embedding for the query and take the first element of the resulting array since we are only embedding one query at a time.\n",
    "\n",
    "        # search in the vector store\n",
    "        try:\n",
    "            results=self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()], # convert the query embedding from a numpy array to a list so that it can be used in the ChromaDB query, which expects embeddings to be in list format.\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            #process results\n",
    "            retrived_docs=[]\n",
    "\n",
    "            if results['documents'] and results['documents'][0]: # check if there are any documents in the results and if the first element of the documents list is not empty\n",
    "                documents=results['documents'][0]\n",
    "                metadatas=results['metadatas'][0]\n",
    "                distances=results['distances'][0] # retrieve the cosine similarity scores (distances) for the retrieved documents from the query results. This can be useful for filtering the results based on relevance and for debugging purposes.\n",
    "                ids=results['ids'][0] # retrieve the list of document IDs from the query results. This can be useful for debugging and for retrieving the original documents later if needed.\n",
    "\n",
    "                for i,(doc_id,document,metadata,distance) in enumerate(zip(ids,documents,metadatas,distances)):  \n",
    "                    # convert distcnce t0o similarity score\n",
    "                    similarity_score=1-distance # since cosine similarity is 1 - cosine distance, we can convert the distance to a similarity score by subtracting the distance from 1. This gives us a similarity score that ranges from 0 to 1, where 1 means the document is identical to the query and 0 means it is completely different.\n",
    "\n",
    "                    if similarity_score>=score_threshold:\n",
    "                        retrived_docs.append(\n",
    "                            {\n",
    "                                'id':doc_id,\n",
    "                                'content':document,\n",
    "                                'metadata':metadata,\n",
    "                                'similarity_score':similarity_score,\n",
    "                                'distance':distance,\n",
    "                                'rank':i+1\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                print(f\"retrived {len(retrived_docs)} documents after filtering\")\n",
    "            else:\n",
    "                print(\"no document found\")\n",
    "\n",
    "            return retrived_docs\n",
    "        except Exception as e:\n",
    "            print(f\"error while retrieving documents: {e}\")\n",
    "            return []\n",
    "               \n",
    "\n",
    "\n",
    "rag_retriver=RAGRetrival(vector_store,embedding_manager)\n",
    "rag_retriver\n",
    " \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d4f5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving document for the query Sparse Attention with top_k=5 and score_threshold=0.0\n",
      "Generating embeddings fro 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3144c1a782a4f07bbaf4a3b21e7523a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(1, 384)\n",
      "retrived 5 documents after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_1e783488_12',\n",
       "  'content': 'Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token',\n",
       "  'metadata': {'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'author': '(anonymous)',\n",
       "   'page': 0,\n",
       "   'title': '(anonymous)',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'format': 'PDF 1.4',\n",
       "   'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'creator': '(unspecified)',\n",
       "   'context_length': 964,\n",
       "   'doc_index': 12,\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'total_pages': 3,\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'source_file': '03_Advanced_Attention_Variants.pdf',\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'subject': '(unspecified)'},\n",
       "  'similarity_score': 0.2736407518386841,\n",
       "  'distance': 0.7263592481613159,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_c9a3a63f_12',\n",
       "  'content': 'Advanced Attention Variants\\nEfficient Attention, Sparse Attention, Flash Attention & Beyond\\n1. The Quadratic Problem\\nStandard self-attention has O(n^2) time and memory complexity with respect to sequence length n. For\\na sequence of 1000 tokens, the attention matrix has 1,000,000 entries. For 10,000 tokens, it has\\n100,000,000 entries. This makes standard attention prohibitively expensive for long documents,\\nhigh-resolution images, or genomic sequences. This challenge spurred a wave of research into efficient\\nattention variants.\\n2. Sparse Attention\\nSparse attention restricts each token to attend to only a subset of other tokens, reducing complexity to\\nO(n * sqrt(n)) or O(n * log(n)). The key insight is that not all token pairs are equally important — most of\\nthe attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token',\n",
       "  'metadata': {'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'source_file': '03_Advanced_Attention_Variants.pdf',\n",
       "   'author': '(anonymous)',\n",
       "   'title': '(anonymous)',\n",
       "   'doc_index': 12,\n",
       "   'trapped': '',\n",
       "   'page': 0,\n",
       "   'creator': '(unspecified)',\n",
       "   'subject': '(unspecified)',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'keywords': '',\n",
       "   'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'context_length': 964,\n",
       "   'total_pages': 3},\n",
       "  'similarity_score': 0.2736407518386841,\n",
       "  'distance': 0.7263592481613159,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_68259f1e_0',\n",
       "  'content': \"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\",\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'author': '(anonymous)',\n",
       "   'source_file': '01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'format': 'PDF 1.4',\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'doc_index': 0,\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'title': '(anonymous)',\n",
       "   'context_length': 924,\n",
       "   'creator': '(unspecified)',\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'total_pages': 3,\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'page': 0,\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'subject': '(unspecified)',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.19435656070709229,\n",
       "  'distance': 0.8056434392929077,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3faf6367_0',\n",
       "  'content': \"Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\\nattention allows neural networks to dynamically weight different parts of an input sequence when\\nproducing an output. Rather than compressing an entire input into a fixed-size vector, attention\\nmechanisms let the model 'look back' at the full input and decide what matters most at each step.\\nThe concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\\nthey showed that allowing the decoder to attend to different parts of the encoder output dramatically\\nimproved translation quality, especially for long sentences. This was a pivotal moment that set the stage\",\n",
       "  'metadata': {'creator': '(unspecified)',\n",
       "   'doc_index': 0,\n",
       "   'total_pages': 3,\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '(unspecified)',\n",
       "   'page': 0,\n",
       "   'source': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'context_length': 924,\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'trapped': '',\n",
       "   'author': '(anonymous)',\n",
       "   'format': 'PDF 1.4',\n",
       "   'keywords': '',\n",
       "   'source_file': '01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\01_Introduction_to_Attention_Mechanisms.pdf',\n",
       "   'title': '(anonymous)',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00'},\n",
       "  'similarity_score': 0.19435656070709229,\n",
       "  'distance': 0.8056434392929077,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_1334ee3b_13',\n",
       "  'content': 'the attention weight is concentrated in a small fraction of positions.\\n2.1 Longformer Attention\\nLongformer (Beltagy et al., 2020) combines local windowed attention with global attention. Each token\\nattends to a local window of w/2 tokens on each side (linear complexity), while special tokens like [CLS]\\nattend globally to all tokens. This allows Longformer to process documents with thousands of tokens\\nefficiently.\\n2.2 BigBird Attention\\nBigBird (Zaheer et al., 2020) uses a combination of random attention (each query attends to r random\\nkeys), window attention (local context), and global tokens. This mixture is theoretically proven to be a\\nuniversal approximator of sequence functions and reduces complexity to O(n).\\n3. Linear Attention\\nLinear attention methods aim to approximate or reformulate softmax attention in O(n) time. The key\\nidea is to decompose the softmax kernel using feature maps phi(x) such that:\\nsoftmax(q^T k) ≈ phi(q)^T phi(k)',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'page': 0,\n",
       "   'subject': '(unspecified)',\n",
       "   'doc_index': 13,\n",
       "   'creator': '(unspecified)',\n",
       "   'creationDate': \"D:20260223110425+00'00'\",\n",
       "   'context_length': 950,\n",
       "   'total_pages': 3,\n",
       "   'keywords': '',\n",
       "   'title': '(anonymous)',\n",
       "   'creationdate': '2026-02-23T11:04:25+00:00',\n",
       "   'format': 'PDF 1.4',\n",
       "   'producer': 'ReportLab PDF Library - (opensource)',\n",
       "   'trapped': '',\n",
       "   'file_path': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'source': '..\\\\data\\\\pdf1\\\\03_Advanced_Attention_Variants.pdf',\n",
       "   'moddate': '2026-02-23T11:04:25+00:00',\n",
       "   'source_file': '03_Advanced_Attention_Variants.pdf',\n",
       "   'modDate': \"D:20260223110425+00'00'\",\n",
       "   'author': '(anonymous)'},\n",
       "  'similarity_score': 0.1705533266067505,\n",
       "  'distance': 0.8294466733932495,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrive(\"Sparse Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5018e",
   "metadata": {},
   "source": [
    "### Integration VectorDB Context Pipeline With LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "779ea9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple rag pipeline using gemini\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# initialize gemini model\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"openai/gpt-oss-20b\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    temperature=0,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "\n",
    "## simple rag functions\n",
    "\n",
    "def rag_simple(query, retriver, llm, top_k=3):\n",
    "    # retrive the context\n",
    "    results = retriver.retrive(query, top_k=top_k)\n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results])\n",
    "\n",
    "    if not context:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    # context + prompt -> llm\n",
    "    prompt = \"\"\"use the following context to answer the question concisely and accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd8d0330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving document for the query what is a attention mechanism? with top_k=3 and score_threshold=0.0\n",
      "Generating embeddings fro 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7440113743184004a6ed956255281f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(1, 384)\n",
      "retrived 3 documents after filtering\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"what is a attention mechanism?\",rag_retriver,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "829aa484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attention mechanism is a neural network component that lets the model dynamically weight and focus on different parts of an input sequence when generating an output. Instead of compressing the entire input into a single fixed‑size vector, attention allows the model to “look back” at the full input and decide which elements are most relevant at each step, improving performance on tasks with long or complex sequences.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97713f",
   "metadata": {},
   "source": [
    "### Enhance RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - returns answer, sources, confidence score and optionally the context used for answering the query\n",
    "    \"\"\"\n",
    "\n",
    "    results = retriever.retrive(query, top_k=top_k, score_threshold=min_score)\n",
    "\n",
    "    if not results:\n",
    "        return {'answer': 'No relevent content found', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "\n",
    "    context = \"\\n\\n\".join(doc['content'] for doc in results)\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('sorce', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "\n",
    "    confidence = max(doc['similarity_score'] for doc in results)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely and accurately.\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer: \n",
    "        \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence,\n",
    "        'context': context if return_context else ''\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b46d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4652b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving document for the query Who first introduced the concept of attention in machine translation, and when? with top_k=5 and score_threshold=0.2\n",
      "Generating embeddings fro 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538119690e0b44dda73bbf0edf1b90b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedings with stage:(1, 384)\n",
      "retrived 2 documents after filtering\n",
      "answer:  Bahdanau et al. introduced the concept in 2014.\n",
      "sources:  [{'source': '01_Introduction_to_Attention_Mechanisms.pdf', 'page': 0, 'score': 0.2522565722465515, 'preview': 'Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring...'}, {'source': '01_Introduction_to_Attention_Mechanisms.pdf', 'page': 0, 'score': 0.2522565722465515, 'preview': 'Introduction to Attention Mechanisms\\nA Comprehensive Overview for Deep Learning Practitioners\\n1. What is Attention?\\nAttention mechanisms are a fundamental component of modern deep learning architectures. Inspired\\nby the human cognitive ability to focus on relevant parts of information while ignoring...'}]\n",
      "confidence:  0.2522565722465515\n",
      "context: Introduction to Attention Mechanisms\n",
      "A Comprehensive Overview for Deep Learning Practitioners\n",
      "1. What is Attention?\n",
      "Attention mechanisms are a fundamental component of modern deep learning architectures. Inspired\n",
      "by the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\n",
      "attention allows neural networks to dynamically weight different parts of an input sequence when\n",
      "producing an output. Rather than compressing an entire input into a fixed-size vector, attention\n",
      "mechanisms let the model 'look back' at the full input and decide what matters most at each step.\n",
      "The concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\n",
      "they showed that allowing the decoder to attend to different parts of the encoder output dramatically\n",
      "improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\n",
      "\n",
      "Introduction to Attention Mechanisms\n",
      "A Comprehensive Overview for Deep Learning Practitioners\n",
      "1. What is Attention?\n",
      "Attention mechanisms are a fundamental component of modern deep learning architectures. Inspired\n",
      "by the human cognitive ability to focus on relevant parts of information while ignoring irrelevant details,\n",
      "attention allows neural networks to dynamically weight different parts of an input sequence when\n",
      "producing an output. Rather than compressing an entire input into a fixed-size vector, attention\n",
      "mechanisms let the model 'look back' at the full input and decide what matters most at each step.\n",
      "The concept was first introduced in the context of machine translation by Bahdanau et al. (2014), where\n",
      "they showed that allowing the decoder to attend to different parts of the encoder output dramatically\n",
      "improved translation quality, especially for long sentences. This was a pivotal moment that set the stage\n"
     ]
    }
   ],
   "source": [
    "#example usage of the advanced RAG function\n",
    "\n",
    "result=rag_advanced(\"Who first introduced the concept of attention in machine translation, and when?\", rag_retriver, llm, top_k=5, min_score=0.2, return_context=True)\n",
    "print(\"answer: \", result[\"answer\"])\n",
    "print(\"sources: \", result[\"sources\"])\n",
    "print(\"confidence: \",result['confidence'])\n",
    "print(\"context:\",result['context'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb3730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_pipeline)",
   "language": "python",
   "name": "rag_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
